[
  {
    "id": 1,
    "question": "In Databricks, which command displays the current Spark catalog?",
    "options": [
      "SHOW CATALOGS",
      "SHOW SCHEMAS",
      "SHOW DATABASES",
      "SHOW TABLES"
    ],
    "correctIndex": 0
  },
  {
    "id": 2,
    "question": "What does the following PySpark code return?",
    "code": "df.selectExpr(\"count(*) as total\").collect()[0][\"total\"]",
    "options": [
      "A DataFrame",
      "A column expression",
      "A single integer",
      "A list of rows"
    ],
    "correctIndex": 2
  },
  {
    "id": 3,
    "question": "Which file format is optimized for Databricks Delta Lake?",
    "options": [
      "CSV",
      "JSON",
      "Parquet",
      "XML"
    ],
    "correctIndex": 2
  },
  {
    "id": 4,
    "question": "A data engineer needs to develop integration tests for an ETL process and deploy a version-controlled, packaged workflow into production using an external job scheduler. Which tool should be used?",
    "options": [
      "Databricks Software Development Kit",
      "Databricks Connect",
      "Databricks Asset Bundles",
      "Databricks Command Line Interface"
    ],
    "correctIndex": 2
  },
  {
    "id": 5,
    "question": "A data engineer streams customer orders into a Kafka topic (orders_topic) and is writing the ingestion script of a DLT pipeline. What is the correct code for ingesting the data?",
    "options": [
      "import dlt\n\n@dlt.table(name=\"orders_raw\")\ndef orders_raw():\n    return (\n        spark.readStream\n            .format(\"kafka\")\n            .option(\"kafka.bootstrap.servers\", \"broker:9092\")\n            .option(\"subscribe\", \"orders_topic\")\n            .option(\"startingOffsets\", \"earliest\")\n            .load()\n    )",
      "CREATE STREAMING LIVE TABLE orders_raw AS SELECT value.order_id AS order_id, value.customer_id AS customer_id, value.amount AS amount, value.order_status AS order_status, value.order_timestamp AS order_timestamp FROM cloud_files(\"kafka://broker:9092/orders_topic\", \"json\");",
      "import dlt\n\n@dlt.table(name=\"orders_raw\")\ndef orders_raw():\n    return (\n        spark.readStream\n            .format(\"cloudFiles\")\n            .option(\"cloudFiles.format\", \"json\")\n            .option(\"cloudFiles.schemaLocation\", \"/schema/location\")\n            .load(\"kafka://broker:9092/orders_topic\")\n    )",
      "CREATE LIVE TABLE orders_raw AS SELECT CAST(json_data AS STRING) AS json_data FROM kafka.kafka_broker:9092.orders_topic;"
    ],
    "correctIndex": 0
  },
  {
    "id": 6,
    "question": "A data engineer writes Python and SQL in the same command cell and tries to use a Python variable inside a SQL SELECT. Why does it fail?",
    "options": [
      "Databricks supports one language per cell.",
      "Databricks supports language interoperability in the same cell but only between Scala and SQL.",
      "Databricks supports multiple languages but only one per notebook.",
      "Databricks supports language interoperability but only if a special character is used."
    ],
    "correctIndex": 0
  },
  {
    "id": 7,
    "question": "A Databricks single-task workflow fails at the last task due to an error in a notebook. The engineer fixes the notebook. What should they do to rerun the workflow?",
    "options": [
      "Repair the task",
      "Rerun the pipeline",
      "Restart the cluster",
      "Switch the cluster"
    ],
    "correctIndex": 0
  },
  {
    "id": 8,
    "question": "A data engineer needs to filter out NULL values in order_datetime from orders_raw and store results in orders_valid using DLT. Which code snippet should be used?",
    "options": [
      "CREATE OR REFRESH STREAMING LIVE TABLE orders_valid CONSTRAINT valid_date EXPECT (order_datetime IS NOT NULL) ON VIOLATION DROP ROW AS SELECT * FROM orders_raw;",
      "CREATE OR REFRESH STREAMING LIVE TABLE orders_valid EXPECT (order_datetime IS NOT NULL) ON VIOLATION DROP ROW AS SELECT * FROM STREAM orders_raw;",
      "CREATE OR REFRESH STREAMING TABLE orders_valid AS SELECT * FROM STREAM orders_raw WHERE order_datetime IS NOT NULL;",
      "CREATE OR REPLACE STREAMING TABLE orders_valid ( FILTER (order_datetime IS NOT NULL) ) AS SELECT * FROM STREAM orders_raw;"
    ],
    "correctIndex": 1
  },
  {
    "id": 9,
    "question": "A data engineer needs to provide access to a group named manufacturing-team. The team needs privileges to create tables in the quality schema. Which SQL commands grant the least privileges?",
    "options": [
      "GRANT CREATE TABLE ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT CREATE SCHEMA ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT CREATE CATALOG ON CATALOG manufacturing TO manufacturing-team;",
      "GRANT CREATE TABLE ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT CREATE SCHEMA ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT USE CATALOG ON CATALOG manufacturing TO manufacturing-team;",
      "GRANT USE TABLE ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT USE SCHEMA ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT USE CATALOG ON CATALOG manufacturing TO manufacturing-team;",
      "GRANT CREATE TABLE ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT USE SCHEMA ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT USE CATALOG ON CATALOG manufacturing TO manufacturing-team;"
    ],
    "correctIndex": 3
  },
  {
    "id": 10,
    "question": "A Python file is ready for production and the workload is small (~10GB) with simple joins. Which cluster is cheapest and efficient?",
    "options": [
      "Interactive cluster",
      "Job cluster with spot instances enabled",
      "Job cluster with spot instances disabled",
      "Job cluster with Photon enabled"
    ],
    "correctIndex": 1
  },
  {
    "id": 11,
    "question": "What is the functionality of Auto Loader in Databricks?",
    "options": [
      "Auto Loader ingests new files from cloud storage, supports only streaming, no schema evolution.",
      "Auto Loader ingests new files from cloud storage, supports batch data with schema evolution.",
      "Auto Loader ingests new files from cloud storage, supports batch and streaming, no schema evolution.",
      "Auto Loader ingests new files from cloud storage, supports batch and streaming with schema evolution."
    ],
    "correctIndex": 3
  },
  {
    "id": 12,
    "question": "An organization needs Databricks to scale automatically for varying workloads to meet SLAs with minimal manual management. Which approach fits?",
    "options": [
      "Use Serverless compute in Databricks to automatically scale and provision resources.",
      "Use Interactive Clusters and adjust sizes manually based on workload.",
      "Use Spot Instances to allocate resources dynamically with potential interruptions.",
      "Deploy Job Clusters with fixed configurations and no auto-scaling."
    ],
    "correctIndex": 0
  },
  {
    "id": 13,
    "question": "How are events formatted in Databricks audit logs?",
    "options": [
      "XML",
      "JSON",
      "CSV",
      "Plain text"
    ],
    "correctIndex": 1
  },
  {
    "id": 14,
    "question": "An organization needs to share a dataset stored in Unity Catalog with an external partner using a different data platform. Which method should be used?",
    "options": [
      "Using Delta Sharing with the open sharing protocol",
      "Exporting data as CSV files and emailing them",
      "Using a third-party API to access the Delta table",
      "Databricks-to-Databricks Sharing"
    ],
    "correctIndex": 0
  },
  {
    "id": 15,
    "question": "A data engineer wants to create an external table that references data stored in ADLS without moving it into Databricks-managed storage. Which step should be taken?",
    "options": [
      "Use CREATE TABLE and specify the LOCATION clause with the path to the external data.",
      "Use CREATE MANAGED TABLE and specify the LOCATION clause with the path to the external data.",
      "CREATE UNMANAGED TABLE without specifying a LOCATION clause.",
      "CREATE EXTERNAL TABLE without specifying a LOCATION clause."
    ],
    "correctIndex": 0
  },
  {
    "id": 16,
    "question": "Why is Delta Live Tables (DLT) an appropriate choice for pipelines that need data quality checks, schema evolution, and easy maintenance?",
    "options": [
      "Automatic data quality checks, built-in support for schema evolution, and declarative pipeline development",
      "Manual schema enforcement, high operational overhead, and limited scalability",
      "Requires custom code for data quality checks, no support for streaming data, and complex pipeline maintenance",
      "Supports only batch processing, no data versioning, and high infrastructure costs"
    ],
    "correctIndex": 0
  },
  {
    "id": 17,
    "question": "Which TWO items are characteristics of the Gold layer? (Choose 2 answers)",
    "options": [
      "Historical lineage",
      "Normalised",
      "Read-optimized",
      "De-normalised",
      "Raw data"
    ],
    "correctIndex": 2
  },
  {
    "id": 18,
    "question": "A data engineer uses Unity Catalog lineage to track data flow. How does lineage support visualization of relationships?",
    "options": [
      "Unity Catalog provides an interactive graph that visualizes dependencies between Delta tables, notebooks, jobs, and dashboards, with column-level tracking.",
      "Unity Catalog lineage supports table-level relationships only and excludes notebooks, jobs, and dashboards.",
      "Unity Catalog lineage tracks dependencies between tables and notebooks but excludes jobs and dashboards.",
      "Unity Catalog lineage visualizes dependencies between Delta tables, notebooks, and jobs but excludes column-level tracing and dashboards."
    ],
    "correctIndex": 0
  },
  {
    "id": 19,
    "question": "A data engineer wants real-time results while developing in a notebook and to reduce cluster spikes. Which cluster meets this need?",
    "options": [
      "Job cluster with autoscaling enabled",
      "All-Purpose Cluster with a large fixed memory size",
      "All-Purpose Cluster with autoscaling",
      "Job cluster with Photon enabled and autoscaling"
    ],
    "correctIndex": 2
  },
  {
    "id": 20,
    "question": "A team ingests Kafka events and wants to store historical event data cost-effectively in a medallion architecture. Where should it be stored?",
    "options": [
      "Gold",
      "Silver",
      "Bronze",
      "Raw layer"
    ],
    "correctIndex": 2
  },
  {
    "id": 21,
    "question": "To set up Delta Sharing with an external partner that uses Unity Catalog, what is the first piece of information to request?",
    "options": [
      "The sharing identifier of their Unity Catalog metastore",
      "Their Databricks account password",
      "The name of their Databricks cluster",
      "The IP address of their Databricks workspace"
    ],
    "correctIndex": 0
  },
  {
    "id": 22,
    "question": "A partner without Databricks needs secure, read-only access to a Delta dataset without creating an account. How should the data be shared?",
    "options": [
      "Share the dataset using Delta Sharing with a secure, read-only URL.",
      "Share using Unity Catalog with full write access.",
      "Export to CSV and transfer the file manually.",
      "Grant partner workspace access with full write permissions."
    ],
    "correctIndex": 0
  },
  {
    "id": 23,
    "question": "A data engineer needs to parse only .png files in a directory using Auto Loader. Which code should be used?",
    "options": [
      "df = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"binaryFile\").load(\"/.png\")",
      "df = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"binaryFile\").option(\"pathGlobFilter\", \".png\").load(\"<base-path>\")",
      "df = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"binaryFile\").append(\"/.png\")",
      "df = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"binaryFile\").option(\"pathGlobFilter\", \".png\").append()"
    ],
    "correctIndex": 1
  },
  {
    "id": 24,
    "question": "A company uses Delta Sharing across clouds and regions. What results in additional costs due to cross-region or egress fees?",
    "options": [
      "Accessing Delta Sharing data using a VPN within the same data center",
      "Transferring data via Delta Sharing across clouds and across different geographic regions",
      "Sharing data within the same cloud provider and region",
      "Using Delta Sharing for internal analytics within a single cloud environment"
    ],
    "correctIndex": 1
  },
  {
    "id": 25,
    "question": "A PySpark DataFrame df has columns product and revenue. Which code computes total revenue, average revenue, and transaction count per product?",
    "options": [
      "from pyspark.sql import functions as F\naggregated_df = df.groupBy(\"product\").agg(F.sum(\"revenue\").alias(\"total_revenue\"), F.avg(\"revenue\").alias(\"avg_revenue\"), F.count(\"*\").alias(\"transaction_count\"))",
      "aggregated_df = df.groupBy(\"product\").agg(sum(\"revenue\"), \"avg(revenue)\", \"count(revenue)\")",
      "from pyspark.sql import functions as F\naggregated_df = df.select(\"product\", \"revenue\").groupBy(\"product\").agg(F.sum(\"revenue\"), F.mean(\"revenue\"))",
      "aggregated_df = df.groupBy(\"product\").agg(f\"revenue\": \"sum\", \"revenue\": \"avg\", \"revenue\": \"count\")"
    ],
    "correctIndex": 0
  },
  {
    "id": 26,
    "question": "A data engineer needs to drop out-of-parameter streaming data without failing the stream. Which DLT feature meets this requirement?",
    "options": [
      "Change Data Capture",
      "Error Handling",
      "Expectations",
      "Monitoring"
    ],
    "correctIndex": 2
  },
  {
    "id": 27,
    "question": "A PySpark ETL job has high CPU time vs Task time. What action should be taken?",
    "options": [
      "Consider executor/core tuning or resizing the cluster for a CPU-bound job.",
      "Repartition data because the cluster is under-utilized.",
      "Increase parallelism due to over-utilized memory.",
      "No change needed; it indicates efficient use."
    ],
    "correctIndex": 0
  },
  {
    "id": 28,
    "question": "Which Databricks feature enables querying external systems (MySQL, Redshift, BigQuery) without ingesting, while maintaining centralized governance?",
    "options": [
      "Databricks Connect",
      "MLflow",
      "Delta Lake",
      "Lakehouse Federation"
    ],
    "correctIndex": 3
  },
  {
    "id": 29,
    "question": "An organization needs an optimized storage layer supporting ACID transactions and schema enforcement. Which technology should be used?",
    "options": [
      "Delta Lake",
      "Unity Catalog",
      "Data lake",
      "Cloud File Storage"
    ],
    "correctIndex": 0
  },
  {
    "id": 30,
    "question": "A streaming ingestion should fail when new columns appear until changes are verified. Which option meets this requirement?",
    "options": [
      "addNewColumns",
      "rescue",
      "failOnNewColumns",
      "none"
    ],
    "correctIndex": 2
  },
  {
    "id": 31,
    "question": "How does Databricks Connect enable local development against Databricks clusters?",
    "options": [
      "Provides a local environment that mimics the Databricks runtime but requires a specific Databricks-only tool.",
      "Provides a local environment that mimics the Databricks runtime, but only through the Databricks web interface.",
      "Allows direct execution of Spark jobs locally without any network connection.",
      "Provides a local environment that mimics the Databricks runtime, enabling development and debugging in a preferred IDE."
    ],
    "correctIndex": 3
  },
  {
    "id": 32,
    "question": "What is the maximum output supported by a job cluster to ensure a notebook does not fail?",
    "options": [
      "15MBs",
      "10MBs",
      "30MBs",
      "25MBs"
    ],
    "correctIndex": 2
  },
  {
    "id": 33,
    "question": "Given sales_df with category and sales_amount, how do you calculate total sales per category into category_sales?",
    "options": [
      "category_sales = sales_df.groupBy(\"category\").agg(sum(\"sales_amount\").alias(\"total_sales_amount\"))",
      "category_sales = sales_df.sum(\"sales_amount\").groupBy(\"category\").alias(\"total_sales_amount\")",
      "category_sales = sales_df.agg(sum(\"sales_amount\").groupBy(\"category\").alias(\"total_sales_amount\"))",
      "category_sales = sales_df.groupBy(\"region\").agg(sum(\"sales_amount\").alias(\"total_sales_amount\"))"
    ],
    "correctIndex": 0
  },
  {
    "id": 34,
    "question": "A Databricks workflow fails at the last stage due to a notebook error. The engineer fixes it and wants to rerun to minimize downtime and cost. What action should they take?",
    "options": [
      "Re-run the entire workflow",
      "Switch to another cluster",
      "Restart the cluster",
      "Repair run"
    ],
    "correctIndex": 3
  },
  {
    "id": 35,
    "question": "Which compute option is best for small ad-hoc Python scripts that run frequently and should wind down quickly?",
    "options": [
      "All-Purpose Cluster",
      "Job Cluster",
      "Serverless Compute",
      "SQL Warehouse"
    ],
    "correctIndex": 1
  },
  {
    "id": 36,
    "question": "A Spark SQL ETL fails with java.lang.OutOfMemoryError: Java heap space. Which two corrective actions should be taken? (Choose 2 answers)",
    "options": [
      "Narrow the filters to collect less data in the query",
      "Upsize the worker nodes and activate autoshuffle partitions",
      "Upsize the driver node and deactivate autoshuffle partitions",
      "Cache the dataset to boost query performance",
      "Fix shuffle partitions to 50 to ensure allocation"
    ],
    "multiSelect": true,
    "correctIndices": [
      0,
      1
    ]
  },
  {
    "id": 37,
    "question": "Which SQL code snippet demonstrates a DDL operation used to create a table?",
    "options": [
      "CREATE TABLE employees (id INT, name STRING, salary DECIMAL(10,2));",
      "ALTER TABLE employees ADD COLUMN new_column INT;",
      "INSERT INTO employees VALUES (1, 'Alice');",
      "SELECT * FROM employees;"
    ],
    "correctIndex": 0
  },
  {
    "id": 38,
    "question": "A Databricks Notebook function sometimes receives wrong input data types. Which feature helps quickly identify incorrect data types?",
    "options": [
      "The Spark UI debug tab contains variables used in the session.",
      "The Databricks debugger provides a variable explorer to inspect values.",
      "Add print statements to check the variable.",
      "The Databricks debugger breakpoints raise an error on wrong data types."
    ],
    "correctIndex": 1
  },
  {
    "id": 39,
    "question": "A PySpark notebook fails during a DataFrame transformation. Which tool lets the engineer set breakpoints and inspect variable values?",
    "options": [
      "Spark UI to analyze execution plans",
      "Databricks CLI to download driver logs",
      "Ganglia UI to monitor cluster resources",
      "Python Notebook Interactive Debugger to inspect variables"
    ],
    "correctIndex": 3
  },
  {
    "id": 40,
    "question": "Which SQL Warehouse should be used for large numbers of queries quickly and cost-effectively within a custom-defined network?",
    "options": [
      "Pro SQL Warehouse",
      "Classic SQL Warehouse",
      "Serverless SQL Warehouse",
      "Serverless compute for notebooks"
    ],
    "correctIndex": 0
  },
  {
    "id": 41,
    "question": "What transformation is typically included when building the Bronze layer?",
    "options": [
      "Business rules and transformations",
      "Include columns like load date/time and process ID",
      "Aggregate data from multiple sources",
      "Perform extensive data cleansing"
    ],
    "correctIndex": 1
  },
  {
    "id": 42,
    "question": "Which syntax loads JSON files as they arrive using Auto Loader and checks it with a DataFrame?",
    "options": [
      "df = spark.read.json(\"/input/path\")",
      "df = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"json\").load(\"/input/path\")",
      "df = spark.readStream.format(\"cloudFiles\").option(\"json\").load(\"/input/path\")",
      "df = spark.read.format(\"cloud\").option(\"json\").load(\"/input/path\")"
    ],
    "correctIndex": 1
  },
  {
    "id": 43,
    "question": "Which Unity Catalog role allows granting/revoking privileges on objects within a schema without read/write access?",
    "options": [
      "Table Owner",
      "USE catalog/schema privilege",
      "Schema Owner",
      "Catalog Owner"
    ],
    "correctIndex": 2
  },
  {
    "id": 44,
    "question": "Which DLT code snippet correctly ingests raw JSON data and creates a Delta table?",
    "options": [
      "import dlt\n\n@dlt.view\ndef raw_customers():\n    return spark.format.json(\"s3://my-bucket/raw-customers/\")",
      "import dlt\n\n@dlt.table\ndef raw_customers():\n    return spark.read.format(\"parquet\").load(\"s3://my-bucket/raw-customers\")",
      "import dlt\n\n@dlt.table\ndef raw_customers():\n    return spark.read.format(\"csv\").load(\"s3://my-bucket/raw-customers/\")",
      "import dlt\n\n@dlt.table\ndef raw_customers():\n    return spark.read.json(\"s3://my-bucket/raw-customers/\")"
    ],
    "correctIndex": 3
  },
  {
    "id": 45,
    "question": "What is the first step in migrating to Databricks Serverless to reduce cost while maintaining SLAs?",
    "options": [
      "Low frequency BI dashboarding and ad-hoc SQL analytics",
      "Legacy ingestion pipelines with APIs, files, JDBC/ODBC",
      "Frequently running Python transformation pipeline compatible with latest runtime and Unity Catalog",
      "Frequently running Scala transformation pipeline compatible with latest runtime and Unity Catalog"
    ],
    "correctIndex": 0
  },
  {
    "id": 46,
    "question": "Which Databricks feature can be used to check data sources and tables used in a workspace?",
    "options": [
      "Do not use lineage because it only tracks the last 3 months.",
      "Use lineage to visualize where the table is used only in notebooks.",
      "Use lineage to visualize where the table is used only in reports.",
      "Use lineage to visualize a graph of all dependencies, including notebooks, tables, and reports."
    ],
    "correctIndex": 3
  },
  {
    "id": 47,
    "question": "What is the primary function of the Silver layer in the Databricks medallion architecture?",
    "options": [
      "Store historical data solely for auditing purposes",
      "Digest raw data in its original state",
      "Aggregate and enrich data for business analytics",
      "Validate, clean, and deduplicate data for further processing"
    ],
    "correctIndex": 3
  },
  {
    "id": 48,
    "question": "How should a data engineer optimize layout for queries filtering by customer_id within date ranges, when the table is partitioned by purchase_date?",
    "options": [
      "Implement liquid clustering by customer_id and purchase_date.",
      "Enable delta caching on the cluster.",
      "Implement liquid clustering on customer_id while keeping existing partitioning.",
      "Partition the table by customer_id."
    ],
    "correctIndex": 2
  },
  {
    "id": 49,
    "question": "A data engineer needs to combine on-prem PostgreSQL data with Azure Synapse data without duplication. What should they use?",
    "options": [
      "Custom ETL pipelines",
      "Export to CSV and upload to Databricks",
      "Manually synchronize into a single database",
      "Lakehouse Federation to query both sources directly"
    ],
    "correctIndex": 3
  },
  {
    "id": 50,
    "question": "To reduce data egress costs when sharing a large dataset from Databricks on AWS to a partner on Azure, which strategy should be used?",
    "options": [
      "Use Delta Sharing without any additional configurations",
      "Configure a VPN between AWS and Azure",
      "Migrate the dataset to Cloudflare R2 object storage before sharing",
      "Share data via pre-signed URLs without monitoring egress costs"
    ],
    "correctIndex": 2
  },
  {
    "id": 51,
    "question": "What is the structure of a Databricks Asset Bundle?",
    "options": [
      "A YAML configuration file specifying artifacts, resources, and configurations",
      "A plain text file listing assets to migrate",
      "A ZIP archive containing only workspace assets without metadata",
      "A Docker image containing runtime environments and source code"
    ],
    "correctIndex": 0
  },
  {
    "id": 52,
    "question": "Which permission should be granted to analysts who can query tables but not modify data?",
    "options": [
      "INSERT",
      "MODIFY",
      "ALL PRIVILEGES",
      "SELECT"
    ],
    "correctIndex": 3
  },
  {
    "id": 53,
    "question": "Which architectural solution meets strict SLAs with minimal runtime errors and avoids cluster management overhead?",
    "options": [
      "Databricks serverless compute",
      "Hybrid scheduled batch jobs on custom VMs",
      "User-managed autoscaling cluster",
      "Dedicated manually managed cluster"
    ],
    "correctIndex": 0
  },
  {
    "id": 54,
    "question": "A data engineer wants to deploy an ETL pipeline from a GitHub repo as a Databricks workflow. Which approach should be used?",
    "options": [
      "Manually create and manage the workflow in the UI",
      "Maintain workflow_config.json and deploy it using Databricks CLI",
      "Maintain workflow_config.json and deploy it using Terraform",
      "Databricks Asset Bundles (DAB) with GitHub integration"
    ],
    "correctIndex": 3
  },
  {
    "id": 55,
    "question": "A data engineer manages multiple external tables and wants least-privilege access per table. How should access be managed?",
    "options": [
      "Grant permissions at the workspace level to cover all external tables",
      "Create a single role with full access to all external tables",
      "Use Unity Catalog to manage access controls for each external table",
      "Set storage permissions at the container level"
    ],
    "correctIndex": 2
  },
  {
    "id": 56,
    "question": "A data engineer needs to share Unity Catalog Delta tables and the notebooks that create them with a partner organization. What is the best approach?",
    "options": [
      "Share codebase via GitHub and let them ingest from your data lake",
      "Share datasets and notebooks via Delta Sharing with permissions managed in Unity Catalog",
      "Zip code and share via email with data ingestion from your data lake",
      "Share data and notebooks using Unity Catalog only"
    ],
    "correctIndex": 1
  },
  {
    "id": 57,
    "question": "For daily batch ETL jobs that are resource-intensive and vary in size, which compute approach is most suitable?",
    "options": [
      "Databricks SQL Serverless",
      "All-Purpose Cluster",
      "Job Cluster",
      "Dedicated Cluster"
    ],
    "correctIndex": 2
  },
  {
    "id": 58,
    "question": "A data engineer wants to step through code in a Databricks Python notebook and inspect variables in real-time. Which tool should be used?",
    "options": [
      "Python Notebook Interactive Debugger",
      "SQL Analytics",
      "Job Execution Dashboard",
      "Cluster Logs"
    ],
    "correctIndex": 0
  },
  {
    "id": 59,
    "question": "Which benefit is provided by array functions in Spark SQL?",
    "options": [
      "Ability to work with data in a variety of types at once",
      "Ability to work with data within certain partitions and windows",
      "Ability to work with time-related data in specified intervals",
      "Ability to work with complex, nested data ingested from JSON files"
    ],
    "correctIndex": 3
  },
  {
    "id": 60,
    "question": "Which of the following is hosted completely in the control plane of the classic Databricks architecture?",
    "options": [
      "Worker node",
      "JDBC data source",
      "Databricks web application",
      "Databricks Filesystem",
      "Driver node"
    ],
    "correctIndex": 2
  },
  {
    "id": 61,
    "question": "Which benefit of the Databricks Lakehouse Platform is provided by Delta Lake?",
    "options": [
      "Ability to manipulate the same data using a variety of languages",
      "Ability to collaborate in real time on a single notebook",
      "Ability to set up alerts for query failures",
      "Ability to support batch and streaming workloads",
      "Ability to distribute complex data operations"
    ],
    "correctIndex": 3
  },
  {
    "id": 62,
    "question": "Which describes the storage organization of a Delta table?",
    "options": [
      "Stored in a single file containing data, history, metadata, and other attributes",
      "Data in a single file, metadata in separate files",
      "Stored in a collection of files containing data, history, metadata, and other attributes",
      "Stored in a collection of files containing only the table data",
      "Stored in a single file containing only the table data"
    ],
    "correctIndex": 2
  },
  {
    "id": 63,
    "question": "Which code removes rows where age > 25 from the Delta table my_table and saves the update?",
    "options": [
      "SELECT * FROM my_table WHERE age > 25;",
      "UPDATE my_table WHERE age > 25;",
      "DELETE FROM my_table WHERE age > 25;",
      "UPDATE my_table WHERE age <= 25;",
      "DELETE FROM my_table WHERE age <= 25;"
    ],
    "correctIndex": 2
  },
  {
    "id": 64,
    "question": "Which tool is used by Auto Loader to process data incrementally?",
    "options": [
      "Checkpointing",
      "Spark Structured Streaming",
      "Databricks SQL",
      "Unity Catalog"
    ],
    "correctIndex": 1
  },
  {
    "id": 65,
    "question": "Which command returns the number of null values in the member_id column?",
    "options": [
      "SELECT count(member_id) FROM my_table;",
      "SELECT count(member_id) - count_null(member_id) FROM my_table;",
      "SELECT count_if(member_id IS NULL) FROM my_table;",
      "SELECT null(member_id) FROM my_table;"
    ],
    "correctIndex": 2
  },
  {
    "id": 66,
    "question": "Which data lakehouse feature improves data quality over a traditional data lake?",
    "options": [
      "Provides storage for structured and unstructured data",
      "Supports ACID-compliant transactions",
      "Allows SQL queries to examine data",
      "Stores data in open formats",
      "Enables ML and AI workloads"
    ],
    "correctIndex": 1
  },
  {
    "id": 67,
    "question": "A data engineer wants a relational object from two tables without storing physical data, and only for the current session. Which object should be created?",
    "options": [
      "Spark SQL Table",
      "View",
      "Delta Table",
      "Temporary view"
    ],
    "correctIndex": 3
  },
  {
    "id": 68,
    "question": "A data engineer left the organization. Who can transfer ownership of their Delta tables in Data Explorer?",
    "options": [
      "Databricks account representative",
      "This transfer is not possible",
      "Workspace administrator",
      "New lead data engineer",
      "Original data engineer"
    ],
    "correctIndex": 2
  },
  {
    "id": 69,
    "question": "Which PySpark command can access a Delta table named sales created in SQL?",
    "options": [
      "SELECT * FROM sales",
      "There is no way to share data between PySpark and SQL.",
      "spark.sql(\"sales\")",
      "spark.delta.table(\"sales\")",
      "spark.table(\"sales\")"
    ],
    "correctIndex": 4
  },
  {
    "id": 70,
    "question": "Which command returns the location of database customer360?",
    "options": [
      "DESCRIBE LOCATION customer360;",
      "DROP DATABASE customer360;",
      "DESCRIBE DATABASE customer360;",
      "ALTER DATABASE customer360 SET DBPROPERTIES ('location' = '/user'};",
      "USE DATABASE customer360;"
    ],
    "correctIndex": 2
  },
  {
    "id": 71,
    "question": "A data engineer needs to create a table of customers in France and include a table property indicating it contains PII. Which line fills the blank?",
    "options": [
      "There is no way to indicate whether a table contains PII.",
      "COMMENT PII",
      "TBLPROPERTIES PII",
      "COMMENT \"Contains PII\"",
      "PII"
    ],
    "correctIndex": 3
  },
  {
    "id": 72,
    "question": "What is stored in the Databricks customer's cloud account?",
    "options": [
      "Databricks web application",
      "Cluster management metadata",
      "Notebooks",
      "Data"
    ],
    "correctIndex": 3
  },
  {
    "id": 73,
    "question": "Which command can write data into a Delta table while avoiding duplicate records?",
    "options": [
      "DROP",
      "IGNORE",
      "MERGE",
      "APPEND",
      "INSERT"
    ],
    "correctIndex": 2
  },
  {
    "id": 74,
    "question": "Files arrive in a shared directory and must be ingested incrementally without deleting them. Which tool solves this?",
    "options": [
      "Unity Catalog",
      "Delta Lake",
      "Databricks SQL",
      "Auto Loader"
    ],
    "correctIndex": 3
  },
  {
    "id": 75,
    "question": "A SQL program should run daily, but the final query should run only on Sundays. Which approach could be used?",
    "options": [
      "Submit a Databricks feature request",
      "Wrap queries in PySpark and use Python control flow to run the final query on Sundays",
      "Run the entire program only on Sundays",
      "Restrict access to the source table so it's only accessible on Sundays",
      "Redesign the model to separate data for the final query"
    ],
    "correctIndex": 1
  },
  {
    "id": 76,
    "question": "COPY INTO transactions from /transactions/raw did not add rows. Why might no new records be copied?",
    "options": [
      "FORMAT_OPTIONS was not used",
      "FILES keyword was not used",
      "The previous day's file was already copied",
      "PARQUET does not support COPY INTO",
      "The table must be refreshed to view copied rows"
    ],
    "correctIndex": 2
  },
  {
    "id": 77,
    "question": "When should MERGE INTO be used instead of INSERT INTO?",
    "options": [
      "When the data location must change",
      "When the target table is external",
      "When the source is not a Delta table",
      "When the target table cannot contain duplicate records"
    ],
    "correctIndex": 3
  },
  {
    "id": 78,
    "question": "Which line fills in the blank to create a table from a SQLite database using JDBC?",
    "options": [
      "org.apache.spark.sql.jdbc",
      "autoloader",
      "org.apache.spark.sql.sqlite",
      "sqlite"
    ],
    "correctIndex": 0
  },
  {
    "id": 79,
    "question": "How can a data engineer identify the owner of new_table?",
    "options": [
      "Review the Permissions tab in the table's page in Data Explorer",
      "There is no way to identify the owner",
      "Review the Owner field in the table's page in Data Explorer",
      "Review the Owner field in the cloud storage solution"
    ],
    "correctIndex": 2
  },
  {
    "id": 80,
    "question": "A DROP TABLE removed metadata but data files still exist. Why?",
    "options": [
      "The table data was larger than 10 GB",
      "The table data was smaller than 10 GB",
      "The table was external",
      "The table did not have a location",
      "The table was managed"
    ],
    "correctIndex": 2
  },
  {
    "id": 81,
    "question": "A data entity must be shared across sessions and saved to a physical location. Which should be created?",
    "options": [
      "Database",
      "Function",
      "View",
      "Temporary view",
      "Table"
    ],
    "correctIndex": 4
  },
  {
    "id": 82,
    "question": "Which tool can automate monitoring of data quality as it degrades on ingestion?",
    "options": [
      "Unity Catalog",
      "Data Explorer",
      "Delta Lake",
      "Delta Live Tables",
      "Auto Loader"
    ],
    "correctIndex": 3
  },
  {
    "id": 83,
    "question": "When migrating to Delta Live Tables with Python for bronze/silver and SQL for gold, what change is needed?",
    "options": [
      "Write the pipeline entirely in Python",
      "Use different notebook sources in SQL and Python",
      "Write the pipeline entirely in SQL",
      "Replace streaming source with batch"
    ],
    "correctIndex": 1
  },
  {
    "id": 84,
    "question": "A job has a complex run schedule and needs to transfer that schedule programmatically. Which tool can represent and submit it?",
    "options": [
      "pyspark.sql.types.DateType",
      "datetime",
      "pyspark.sql.types.TimestampType",
      "Cron syntax"
    ],
    "correctIndex": 3
  },
  {
    "id": 85,
    "question": "Which operation lets a Python-based team run a SQL query and work with the results in PySpark?",
    "options": [
      "SELECT * FROM sales",
      "spark.delta.table",
      "spark.sql",
      "spark.table"
    ],
    "correctIndex": 2
  },
  {
    "id": 86,
    "question": "How does a data lakehouse help reduce discrepancies between data engineering and analysis reports?",
    "options": [
      "Teams respond more quickly to ad-hoc requests",
      "Both teams use the same source of truth",
      "Both teams reorganize to the same department",
      "Teams collaborate in real time"
    ],
    "correctIndex": 1
  },
  {
    "id": 87,
    "question": "Which Structured Streaming query performs a hop from Silver to Gold?",
    "options": [
      "spark.readstream.load(rawSalesLocation).writeStream.option(\"checkpointLocation\", checkpointPath).outputMode(\"append\").table(\"newsales\")",
      "spark.table(\"sales\").withColumn(\"avgPrice\", col(\"sales\") / col(\"units\")).writeStream.option(\"checkpointLocation\", checkpointPath).outputMode(\"append\").table(\"newsales\")",
      "spark.table(\"sales\").filter(col(\"units\") > 0).writeStream.option(\"checkpointLocation\", checkpointPath).outputMode(\"append\").table(\"newsales\")",
      "spark.table(\"sales\").groupBy(\"store\").agg(sum(\"sales\")).writeStream.option(\"checkpointLocation\", checkpointPath).outputMode(\"complete\").table(\"newsales\")"
    ],
    "correctIndex": 3
  },
  {
    "id": 88,
    "question": "Which trigger configuration runs a micro-batch every 5 seconds?",
    "options": [
      "trigger(\"5 seconds\")",
      "trigger()",
      "seconds()",
      "trigger(processingTime=\"5 seconds\")",
      "seconds(\")"
    ],
    "correctIndex": 3
  },
  {
    "id": 89,
    "question": "DLT expectation: CONSTRAINT valid_timestamp EXPECT (timestamp > '2020-01-01') ON VIOLATION DROP ROW. What happens to violating records?",
    "options": [
      "Dropped from target and loaded into a quarantine table",
      "Added to target and flagged invalid",
      "Dropped from target and recorded as invalid in the event log",
      "Added to target and recorded as invalid in the event log",
      "Job fails"
    ],
    "correctIndex": 2
  },
  {
    "id": 90,
    "question": "When should CREATE STREAMING LIVE TABLE be used instead of CREATE LIVE TABLE?",
    "options": [
      "When the next DLT step is static",
      "When data needs to be processed incrementally",
      "It is redundant and never needed",
      "When data requires complicated aggregations",
      "When the previous DLT step is static"
    ],
    "correctIndex": 1
  },
  {
    "id": 91,
    "question": "In a DLT query using STREAM(LIVE.customers), why is STREAM included?",
    "options": [
      "It is not needed and will cause an error",
      "The customers table has been updated since the last run",
      "The customers table is a streaming live table",
      "The customers table references a Structured Streaming query on a PySpark DataFrame"
    ],
    "correctIndex": 2
  },
  {
    "id": 92,
    "question": "Which Git operation must be performed outside of Databricks Repos?",
    "options": [
      "Commit",
      "Pull",
      "Merge",
      "Clone"
    ],
    "correctIndex": 2
  },
  {
    "id": 93,
    "question": "In a DLT pipeline with expectations dropping invalid records, how can you identify where records are dropped?",
    "options": [
      "Set separate expectations for each table",
      "It is not possible",
      "Enable email notifications for dropped records",
      "Use the DLT pipeline page and view data quality statistics per table",
      "Use the Error button to review present errors"
    ],
    "correctIndex": 3
  },
  {
    "id": 94,
    "question": "A single-task Job needs a new notebook to run before the existing task. How should this be set up?",
    "options": [
      "Clone the existing task and update it",
      "Create a new task and add it as a dependency of the original task",
      "Create a new task and add the original task as its dependency",
      "Create a new job from scratch and run both tasks concurrently",
      "Clone the task into a new job and edit it"
    ],
    "correctIndex": 1
  },
  {
    "id": 95,
    "question": "A SQL query should refresh every minute for a week, then stop to avoid costs. What should be done?",
    "options": [
      "Set a DBU limit for the SQL endpoint",
      "Set the refresh schedule to end after a number of refreshes",
      "It cannot be ensured",
      "Limit who can manage the refresh schedule",
      "Set the refresh schedule to end on a certain date"
    ],
    "correctIndex": 4
  },
  {
    "id": 96,
    "question": "Which command creates all_transactions with all records from March and April without duplicates?",
    "options": [
      "CREATE TABLE all_transactions AS SELECT * FROM march_transactions INNER JOIN SELECT * FROM april_transactions;",
      "CREATE TABLE all_transactions AS SELECT * FROM march_transactions UNION SELECT * FROM april_transactions;",
      "CREATE TABLE all_transactions AS SELECT * FROM march_transactions OUTER JOIN SELECT * FROM april_transactions;",
      "CREATE TABLE all_transactions AS SELECT * FROM march_transactions INTERSECT SELECT * from april_transactions;"
    ],
    "correctIndex": 1
  },
  {
    "id": 97,
    "question": "To minimize SQL endpoint runtime for a daily dashboard refresh, what should be enabled?",
    "options": [
      "Ensure the dashboard endpoint matches each query endpoint",
      "Set the dashboard endpoint to serverless",
      "Turn on Auto Stop for the SQL endpoint",
      "Reduce the cluster size of the SQL endpoint",
      "Ensure the dashboard endpoint is not one of the query endpoints"
    ],
    "correctIndex": 2
  },
  {
    "id": 98,
    "question": "When should a new Databricks Job Task select another task in the Depends On field?",
    "options": [
      "When another task needs to be replaced",
      "When another task must successfully complete before the new task begins",
      "When another task has the same dependency libraries",
      "When another task needs to use minimal compute"
    ],
    "correctIndex": 1
  },
  {
    "id": 99,
    "question": "What must be specified when creating a new Delta Live Tables pipeline?",
    "options": [
      "A key-value pair configuration",
      "At least one notebook library to be executed",
      "A cloud storage path for written data",
      "A target database location for written data"
    ],
    "correctIndex": 1
  },
  {
    "id": 100,
    "question": "A multi-task job runs slowly because clusters take long to start. What can improve startup time?",
    "options": [
      "Use Databricks SQL endpoints",
      "Use jobs clusters instead of all-purpose clusters",
      "Configure single-node clusters",
      "Use clusters from a cluster pool",
      "Enable autoscaling for larger data sizes"
    ],
    "correctIndex": 3
  },
  {
    "id": 101,
    "question": "Which command grants full privileges on database customers to a team?",
    "options": [
      "GRANT USAGE ON DATABASE customers TO team;",
      "GRANT ALL PRIVILEGES ON DATABASE team TO customers;",
      "GRANT SELECT PRIVILEGES ON DATABASE customers TO teams;",
      "GRANT SELECT CREATE MODIFY USAGE PRIVILEGES ON DATABASE customers TO team;",
      "GRANT ALL PRIVILEGES ON DATABASE customers TO team;"
    ],
    "correctIndex": 4
  },
  {
    "id": 102,
    "question": "Which command grants a team permission to see what tables exist in database customers?",
    "options": [
      "GRANT VIEW ON CATALOG customers TO team;",
      "GRANT CREATE ON DATABASE customers TO team;",
      "GRANT USAGE ON CATALOG team TO customers;",
      "GRANT CREATE ON DATABASE team TO customers;",
      "GRANT USAGE ON DATABASE customers TO team;"
    ],
    "correctIndex": 4
  },
  {
    "id": 103,
    "question": "A Databricks Repo needs updates from the central Git repository. Which Git operation is required?",
    "options": [
      "Merge",
      "Push",
      "Pull",
      "Commit",
      "Clone"
    ],
    "correctIndex": 2
  },
  {
    "id": 104,
    "question": "Which benefit comes from the Databricks Lakehouse Platform embracing open source technologies?",
    "options": [
      "Cloud-specific integrations",
      "Simplified governance",
      "Ability to scale storage",
      "Ability to scale workloads",
      "Avoiding vendor lock-in"
    ],
    "correctIndex": 4
  },
  {
    "id": 105,
    "question": "Which control flow statement begins a block that runs when day_of_week == 1 and review_period is True?",
    "options": [
      "if day_of_week = 1 and review_period:",
      "if day_of_week = 1 and review_period = \"True\":",
      "if day_of_week = 1 & review_period: = \"True\":",
      "if day_of_week == 1 and review_period:"
    ],
    "correctIndex": 3
  },
  {
    "id": 106,
    "question": "When is a single-node cluster most appropriate?",
    "options": [
      "Working interactively with a small amount of data",
      "Running automated reports to refresh as quickly as possible",
      "Working with SQL in Databricks SQL",
      "Needing automatic scaling with larger data",
      "Running reports manually with a large amount of data"
    ],
    "correctIndex": 0
  },
  {
    "id": 107,
    "question": "Which describes the relationship between Bronze tables and raw data?",
    "options": [
      "Bronze tables contain less data than raw data files",
      "Bronze tables contain more truthful data than raw data",
      "Bronze tables contain raw data with a schema applied",
      "Bronze tables contain a less refined view of data than raw data"
    ],
    "correctIndex": 2
  },
  {
    "id": 108,
    "question": "Which keyword can be used to compact small files for a Delta table?",
    "options": [
      "REDUCE",
      "OPTIMIZE",
      "COMPACTION",
      "REPARTITION",
      "VACUUM"
    ],
    "correctIndex": 1
  },
  {
    "id": 109,
    "question": "A data engineer cannot time travel to a 3-day-old version because data files were deleted. Why?",
    "options": [
      "VACUUM was run on the table",
      "TIME TRAVEL was run on the table",
      "DELETE HISTORY was run on the table",
      "OPTIMIZE was run on the table"
    ],
    "correctIndex": 0
  },
  {
    "id": 110,
    "question": "Which code block creates a SQL UDF to apply custom logic to a city string?",
    "options": [
      "CREATE FUNCTION combine_nyc(city STRING) RETURNS STRING RETURN CASE WHEN city = 'brooklyn' THEN 'new york' ELSE city END;",
      "CREATE VDF combine_nyc(city STRING) RETURNS STRING CASE WHEN city = 'brooklyn' THEN 'new york' ELSE city END;",
      "CREATE FUNCTION combine_nyc(city STRING) RETURNS STRING CASE WHEN city = 'brooklyn' THEN 'new york' ELSE city;",
      "CREATE RETURNS STRING combine_nyc city WHEN city = 'brooklyn' THEN 'new york' END;"
    ],
    "correctIndex": 0
  },
  {
    "id": 111,
    "question": "Which can simplify and unify siloed data architectures specialized for specific use cases?",
    "options": [
      "None of these",
      "Data lake",
      "Data warehouse",
      "All of these",
      "Data lakehouse"
    ],
    "correctIndex": 4
  },
  {
    "id": 112,
    "question": "How can a manager ensure a Databricks SQL query refreshes daily?",
    "options": [
      "Schedule refresh every 1 day from the SQL endpoint's page",
      "Schedule refresh every 12 hours from the SQL endpoint's page",
      "Schedule refresh every 1 day from the query's page",
      "Schedule the query every 12 hours from the Jobs UI"
    ],
    "correctIndex": 2
  },
  {
    "id": 113,
    "question": "How can SQL be used within a cell of a Python notebook without changing other cells?",
    "options": [
      "It is not possible to use SQL in a Python notebook",
      "Attach the cell to a SQL endpoint",
      "Simply write SQL syntax in the cell",
      "Add %sql to the first line of the cell",
      "Change the notebook default language to SQL"
    ],
    "correctIndex": 3
  },
  {
    "id": 114,
    "question": "Which SQL keyword converts a table from long format to wide format?",
    "options": [
      "TRANSFORM",
      "PIVOT",
      "SUM",
      "CONVERT",
      "WHERE"
    ],
    "correctIndex": 1
  },
  {
    "id": 115,
    "question": "Which benefit applies to creating an external table from Parquet rather than CSV using CTAS?",
    "options": [
      "Parquet files can be partitioned",
      "CTAS cannot be used on files",
      "Parquet files have a well-defined schema",
      "Parquet files can be optimized",
      "Parquet files will become Delta tables"
    ],
    "correctIndex": 2
  },
  {
    "id": 116,
    "question": "Which method ensures Workflows are triggered on schedule?",
    "options": [
      "Scheduled Workflows require an always-running cluster",
      "Scheduled Workflows process data as it arrives at configured sources",
      "Scheduled Workflows can reduce costs since clusters run only long enough to execute",
      "Scheduled Workflows run continuously until manually stopped"
    ],
    "correctIndex": 2
  },
  {
    "id": 117,
    "question": "A data engineer has USAGE on catalog and schema. What minimum permission is needed to access a view and its data?",
    "options": [
      "SELECT on the view and the underlying table",
      "SELECT only on the view",
      "ALL PRIVILEGES on the view",
      "ALL PRIVILEGES at the schema level"
    ],
    "correctIndex": 0
  },
  {
    "id": 118,
    "question": "Which query uses FILTER to keep employees with years_exp > 5 in an array column?",
    "options": [
      "SELECT store_id, FILTER(employees, e -> e.years_exp > 5) AS exp_employees FROM stores;",
      "SELECT store_id, FILTER(employees, years_exp > 5) AS exp_employees FROM stores;",
      "SELECT store_id, FILTER(employees, e.years_exp) AS exp_employees FROM stores;",
      "SELECT store_id, CASE WHEN employees.years_exp > 5 THEN employees END AS exp_employees FROM stores;"
    ],
    "correctIndex": 0
  },
  {
    "id": 119,
    "question": "A DLT pipeline must fail if location is NULL. Which constraint implements this?",
    "options": [
      "CONSTRAINT valid_location EXPECT (location = NULL)",
      "CONSTRAINT valid_location EXPECT (location != NULL) ON VIOLATION FAIL UPDATE",
      "CONSTRAINT valid_location EXPECT (location != NULL) ON DROP ROW",
      "CONSTRAINT valid_location EXPECT (location != NULL) ON VIOLATION FAIL"
    ],
    "correctIndex": 1
  },
  {
    "id": 120,
    "question": "Which table should be created to store Parquet data at a specific external path?",
    "options": [
      "An external table with LOCATION pointing to the specific external path",
      "An external table where schema has managed location pointing to the path",
      "A managed table where the catalog has managed location pointing to the path",
      "A managed table with LOCATION pointing to the external path"
    ],
    "correctIndex": 0
  },
  {
    "id": 121,
    "question": "When migrating a mixed Python/SQL pipeline to DLT, which change is needed?",
    "options": [
      "The pipeline can have different notebook sources in SQL and Python",
      "The pipeline must be written entirely in SQL",
      "The pipeline must use a batch source",
      "The pipeline must be written entirely in Python"
    ],
    "correctIndex": 0
  },
  {
    "id": 122,
    "question": "Which Python function definition adds two integers and returns the sum?",
    "options": [
      "def add_integers(x, y):\n    return x + y",
      "def add_integers(x, y):\n    print(x + y)",
      "function add_integers(x, y)\n    return x + y",
      "def add_integers(x, y):\n    return x + Y"
    ],
    "correctIndex": 0
  },
  {
    "id": 123,
    "question": "To process all available data in as many batches as required, which trigger should be used?",
    "options": [
      "trigger(availableNow=True)",
      "trigger(processingTime=\"once\")",
      "trigger(continuous=\"once\")",
      "trigger(once=True)"
    ],
    "correctIndex": 0
  },
  {
    "id": 124,
    "question": "What can simplify and unify siloed data architectures specialized for specific use cases?",
    "options": [
      "Delta Lake",
      "Data lake",
      "Data warehouse",
      "Data lakehouse"
    ],
    "correctIndex": 3
  },
  {
    "id": 125,
    "question": "How should an analyst query a Delta table version from two weeks ago?",
    "options": [
      "Truncate and reload the table from two weeks ago",
      "Find the version in the Delta log and query using VERSION AS OF (or export that version)",
      "RESTORE the table to two weeks ago and query",
      "Run VACUUM to remove versions older than two weeks"
    ],
    "correctIndex": 1
  },
  {
    "id": 126,
    "question": "Why did Auto Loader infer all columns as STRING when ingesting JSON without schema hints?",
    "options": [
      "There was a type mismatch between schemas",
      "JSON is a text-based format",
      "Auto Loader only works with string data",
      "All fields had at least one null",
      "Auto Loader cannot infer schema"
    ],
    "correctIndex": 1
  },
  {
    "id": 127,
    "question": "A DLT pipeline in Development mode using Continuous mode is started. What is the expected outcome?",
    "options": [
      "All datasets update once, pipeline shuts down, compute terminated",
      "All datasets update at intervals until shut down, compute persists",
      "All datasets update once and pipeline persists idle with compute",
      "All datasets update once and pipeline shuts down, compute persists",
      "All datasets update at intervals until shut down, compute persists for testing"
    ],
    "correctIndex": 4
  },
  {
    "id": 128,
    "question": "Which workload uses a Gold table as its source?",
    "options": [
      "Parsing timestamps into a human-readable format",
      "Aggregating uncleaned data for summary statistics",
      "Cleaning data by removing malformed records",
      "Querying aggregated data for a dashboard",
      "Ingesting raw streaming data"
    ],
    "correctIndex": 3
  },
  {
    "id": 129,
    "question": "Which two components are in the Databricks platform control plane? (Choose 2 answers)",
    "options": [
      "Virtual Machines",
      "Compute Orchestration",
      "Serverless Compute",
      "Compute",
      "Unity Catalog"
    ],
    "multiSelect": true,
    "correctIndices": [
      1,
      4
    ]
  },
  {
    "id": 130,
    "question": "Given values [0, 1, 2, NULL, 2, 3], what is the output of count_if(col > 1), count(*), count(col)?",
    "options": [
      "3 6 5",
      "4 6 5",
      "3 6 6",
      "4 6 6"
    ],
    "correctIndex": 0
  },
  {
    "id": 131,
    "question": "Which workload type is always compatible with Auto Loader?",
    "options": [
      "Streaming workloads",
      "Machine learning workloads",
      "Serverless workloads",
      "Batch workloads",
      "Dashboard workloads"
    ],
    "correctIndex": 0
  },
  {
    "id": 132,
    "question": "A tested Python notebook should be scheduled in production. Which cluster is best?",
    "options": [
      "All-purpose cluster",
      "Any Unity Catalog-enabled cluster",
      "Jobs cluster",
      "Serverless SQL warehouse"
    ],
    "correctIndex": 2
  },
  {
    "id": 133,
    "question": "A batch ingestion reads from a Delta table. What change is needed to read the table as a stream source?",
    "options": [
      "Replace predict with a stream-friendly prediction function",
      "Replace schema(schema) with option(\"maxFilesPerTrigger\", 1)",
      "Replace the table name with a path",
      "Replace format(\"delta\") with format(\"stream\")",
      "Replace spark.read with spark.readStream"
    ],
    "correctIndex": 4
  },
  {
    "id": 134,
    "question": "DLT expectation: CONSTRAINT valid_timestamp EXPECT (timestamp > '2020-01-01') ON VIOLATION FAIL UPDATE. What happens to violating records?",
    "options": [
      "Dropped from target and recorded in the event log",
      "Job fails",
      "Dropped and loaded into a quarantine table",
      "Added to target and recorded as invalid in the event log",
      "Added to target and flagged invalid"
    ],
    "correctIndex": 1
  },
  {
    "id": 135,
    "question": "Which statement about Silver vs Bronze tables is always true?",
    "options": [
      "Silver tables are less refined than Bronze",
      "Silver tables are aggregated while Bronze is unaggregated",
      "Silver tables contain more data than Bronze",
      "Silver tables contain a more refined and cleaner view than Bronze",
      "Silver tables contain less data than Bronze"
    ],
    "correctIndex": 3
  },
  {
    "id": 136,
    "question": "SQL queries are slow when submitted to a non-running SQL endpoint. How can startup time be reduced?",
    "options": [
      "Enable Serverless and set Spot Instance Policy to Reliability Optimized",
      "Turn on Auto Stop",
      "Increase the cluster size",
      "Enable Serverless for the SQL endpoint",
      "Increase the maximum scaling range"
    ],
    "correctIndex": 3
  },
  {
    "id": 137,
    "question": "Which command grants full privileges on table sales to a team?",
    "options": [
      "GRANT ALL PRIVILEGES ON TABLE sales TO team;",
      "GRANT SELECT CREATE MODIFY ON TABLE sales TO team;",
      "GRANT SELECT ON TABLE sales TO team;",
      "GRANT ALL PRIVILEGES ON TABLE team TO sales;"
    ],
    "correctIndex": 0
  },
  {
    "id": 138,
    "question": "How can a Job owner be emailed on job failure?",
    "options": [
      "Manually program alerts in each notebook cell",
      "Set up an Alert in the Job page",
      "Set up an Alert in the Notebook",
      "There is no way to notify the Job owner",
      "MLflow Model Registry Webhooks"
    ],
    "correctIndex": 1
  },
  {
    "id": 139,
    "question": "Which command grants a team permission to see tables in database customers?",
    "options": [
      "GRANT VIEW ON CATALOG customers TO team;",
      "GRANT CREATE ON DATABASE customers TO team;",
      "GRANT USAGE ON CATALOG team TO customers;",
      "GRANT USAGE ON DATABASE customers TO team;"
    ],
    "correctIndex": 3
  },
  {
    "id": 140,
    "question": "How can a SQL query refresh every minute for a week and stop to avoid costs?",
    "options": [
      "Set a DBU limit",
      "End the refresh schedule after a number of refreshes",
      "End the refresh schedule on a certain date",
      "Limit who can manage the schedule"
    ],
    "correctIndex": 2
  },
  {
    "id": 141,
    "question": "How can a team be notified via webhook when a SQL query result exceeds a threshold?",
    "options": [
      "Set up an Alert with a custom template",
      "Set up an Alert with a new email destination",
      "Set up an Alert with one-time notifications",
      "Set up an Alert with a new webhook alert destination",
      "Set up an Alert without notifications"
    ],
    "correctIndex": 3
  },
  {
    "id": 142,
    "question": "To minimize SQL endpoint runtime for hourly dashboard refreshes, what should be enabled?",
    "options": [
      "Turn on Auto Stop for the SQL endpoint",
      "Ensure the dashboard endpoint is not one of the query endpoints",
      "Reduce the cluster size",
      "Ensure the dashboard endpoint matches each query endpoint",
      "Set the dashboard endpoint to serverless"
    ],
    "correctIndex": 0
  },
  {
    "id": 143,
    "question": "Which two conditions apply for Unity Catalog governance? (Choose 2 answers)",
    "options": [
      "You can have more than 1 metastore per account, but only 1 per region",
      "Both catalog and schema must have managed locations if the metastore has no location",
      "You can have multiple catalogs per metastore and one catalog can be in multiple metastores",
      "If a catalog has no location, the schema must have a managed location",
      "If the metastore has no location, the catalog must have a managed location"
    ],
    "multiSelect": true,
    "correctIndices": [
      0,
      3
    ]
  },
  {
    "id": 144,
    "question": "To minimize SQL endpoint runtime for daily dashboard refreshes, what should be enabled?",
    "options": [
      "Ensure the dashboard endpoint matches each query endpoint",
      "Set the dashboard endpoint to serverless",
      "Turn on Auto Stop for the SQL endpoint",
      "Ensure the dashboard endpoint is not one of the query endpoints"
    ],
    "correctIndex": 2
  },
  {
    "id": 145,
    "question": "In which scenario should a data team use cluster pools?",
    "options": [
      "Automated report needs version control across collaborators",
      "Automated report needs to be runnable by all stakeholders",
      "Automated report needs to be refreshed as quickly as possible",
      "Automated report needs to be reproducible"
    ],
    "correctIndex": 2
  },
  {
    "id": 146,
    "question": "What is hosted completely in the control plane of the classic Databricks architecture?",
    "options": [
      "Worker node",
      "Databricks web application",
      "Driver node",
      "Databricks Filesystem"
    ],
    "correctIndex": 1
  },
  {
    "id": 147,
    "question": "What is an advantage of Databricks Repos over notebook versioning?",
    "options": [
      "Allows reverting to previous notebook versions",
      "Is wholly housed within the Databricks Data Intelligence Platform",
      "Provides the ability to comment on specific changes",
      "Supports multiple branches"
    ],
    "correctIndex": 3
  },
  {
    "id": 148,
    "question": "What is a benefit of the Databricks Lakehouse Architecture embracing open source technologies?",
    "options": [
      "Avoiding vendor lock-in",
      "Simplified governance",
      "Ability to scale workloads",
      "Cloud-specific integrations"
    ],
    "correctIndex": 0
  },
  {
    "id": 149,
    "question": "Where can a data engineer review their permissions on a Delta table?",
    "options": [
      "Jobs",
      "Dashboards",
      "Catalog Explorer",
      "Repos"
    ],
    "correctIndex": 2
  },
  {
    "id": 150,
    "question": "A Databricks Repo needs updates from the central Git repository. Which operation is required?",
    "options": [
      "Clone",
      "Pull",
      "Merge",
      "Push"
    ],
    "correctIndex": 1
  },
  {
    "id": 151,
    "question": "Which file format is used to store Delta Lake tables?",
    "options": [
      "CSV",
      "Parquet",
      "JSON",
      "Delta"
    ],
    "correctIndex": 1
  },
  {
    "id": 152,
    "question": "Which SQL command appends ('a1', 6, 9.4) to an existing Delta table my_table?",
    "options": [
      "INSERT INTO my_table VALUES ('a1', 6, 9.4)",
      "INSERT VALUES ('a1', 6, 9.4) INTO my_table",
      "UPDATE my_table VALUES ('a1', 6, 9.4)",
      "UPDATE VALUES ('a1', 6, 9.4) my_table"
    ],
    "correctIndex": 0
  },
  {
    "id": 153,
    "question": "Which keyword compacts small files in a Delta table?",
    "options": [
      "OPTIMIZE",
      "VACUUM",
      "COMPACTION",
      "REPARTITION"
    ],
    "correctIndex": 0
  },
  {
    "id": 154,
    "question": "Which PySpark command can access the Delta table sales created in SQL?",
    "options": [
      "SELECT * FROM sales",
      "spark.table(\"sales\")",
      "spark.sql(\"sales\")",
      "spark.delta.table(\"sales\")"
    ],
    "correctIndex": 1
  },
  {
    "id": 155,
    "question": "Where is a database created with CREATE DATABASE IF NOT EXISTS customer360 located by default?",
    "options": [
      "dbfs:/user/hive/database/customer360",
      "dbfs:/user/hive/warehouse",
      "dbfs:/user/hive/customer360",
      "dbfs:/user/hive/database"
    ],
    "correctIndex": 1
  },
  {
    "id": 156,
    "question": "DROP TABLE IF EXISTS my_table removed data and metadata files. Why?",
    "options": [
      "The table was managed",
      "The table data was smaller than 10 GB",
      "The table did not have a location",
      "The table was external"
    ],
    "correctIndex": 0
  },
  {
    "id": 157,
    "question": "Which line fills the blank to create a table from CSV data at /path/to/csv?",
    "options": [
      "FROM \"path/to/csv\"",
      "USING CSV",
      "FROM CSV",
      "USING DELTA"
    ],
    "correctIndex": 1
  },
  {
    "id": 158,
    "question": "Which SQL keyword can convert a table from long format to wide format?",
    "options": [
      "TRANSFORM",
      "PIVOT",
      "SUM",
      "CONVERT"
    ],
    "correctIndex": 1
  },
  {
    "id": 159,
    "question": "Which function fills the blank to execute a SQL query using table_name?",
    "options": [
      "spark.delta.sql",
      "spark.sql",
      "spark.table",
      "dbutils.sql"
    ],
    "correctIndex": 1
  },
  {
    "id": 160,
    "question": "Which trigger runs a micro-batch every 5 seconds?",
    "options": [
      "trigger(\"5 seconds\")",
      "trigger(continuous=\"5 seconds\")",
      "trigger(once=\"5 seconds\")",
      "trigger(processingTime=\"5 seconds\")"
    ],
    "correctIndex": 3
  },
  {
    "id": 161,
    "question": "How can a DLT pipeline identify which table is dropping invalid records?",
    "options": [
      "Set separate expectations for each table",
      "Use the Error button to review errors",
      "Enable email notifications",
      "View data quality statistics per table in the pipeline page"
    ],
    "correctIndex": 3
  },
  {
    "id": 162,
    "question": "What is used to record offset ranges so Structured Streaming can track progress and recover?",
    "options": [
      "Checkpointing and Write-ahead Logs",
      "Replayable Sources and Idempotent Sinks",
      "Write-ahead Logs and Idempotent Sinks",
      "Checkpointing and Idempotent Sinks"
    ],
    "correctIndex": 3
  },
  {
    "id": 163,
    "question": "What describes the relationship between Gold and Silver tables?",
    "options": [
      "Gold tables are more likely to contain aggregations than Silver tables",
      "Gold tables are more likely to contain valuable data than Silver tables",
      "Gold tables are less refined than Silver tables",
      "Gold tables are more truthful than Silver tables"
    ],
    "correctIndex": 0
  },
  {
    "id": 164,
    "question": "A DLT pipeline runs in Production mode with Continuous mode. What happens after Start?",
    "options": [
      "Datasets update at intervals; compute persists for testing",
      "Datasets update once; compute persists for testing",
      "Datasets update at intervals; compute is deployed and terminated when stopped",
      "Datasets update once; compute terminated"
    ],
    "correctIndex": 2
  },
  {
    "id": 165,
    "question": "Which workloads are compatible with Auto Loader?",
    "options": [
      "Streaming workloads",
      "Machine learning workloads",
      "Serverless workloads",
      "Batch workloads"
    ],
    "correctIndex": 0
  },
  {
    "id": 166,
    "question": "Why did Auto Loader infer all columns as STRING without schema hints?",
    "options": [
      "Auto Loader cannot infer schema",
      "JSON is a text-based format",
      "Auto Loader only works with string data",
      "All fields had at least one null"
    ],
    "correctIndex": 1
  },
  {
    "id": 167,
    "question": "A multi-task job starts slowly because clusters take time to start. What improves startup time?",
    "options": [
      "Use Databricks SQL endpoints",
      "Use jobs clusters instead of all-purpose clusters",
      "Enable autoscaling for larger data sizes",
      "Use clusters from a cluster pool"
    ],
    "correctIndex": 3
  },
  {
    "id": 168,
    "question": "A new task must run before an existing single-task job. How should this be set up?",
    "options": [
      "Clone the existing task and update it",
      "Create a new task and set it as a dependency of the original task",
      "Create a new task and set the original task as its dependency",
      "Create a new job from scratch and run both tasks concurrently"
    ],
    "correctIndex": 1
  },
  {
    "id": 169,
    "question": "A job has two notebook tasks; one is slow. Where should a tech lead look to diagnose?",
    "options": [
      "Runs tab and immediately review the processing notebook",
      "Tasks tab and click the active run to review the processing notebook",
      "Runs tab and click the active run to review the processing notebook",
      "Tasks tab to immediately review the processing notebook"
    ],
    "correctIndex": 2
  },
  {
    "id": 170,
    "question": "SQL queries are slow on an always-on endpoint with many concurrent small queries. What helps?",
    "options": [
      "Increase the cluster size",
      "Increase the maximum bound of the scaling range",
      "Turn on Auto Stop",
      "Turn on Serverless"
    ],
    "correctIndex": 1
  },
  {
    "id": 171,
    "question": "How can a team be notified via webhook when a SQL query result reaches a threshold?",
    "options": [
      "Alert with a custom template",
      "Alert with a new email destination",
      "Alert with a new webhook destination",
      "Alert with one-time notifications"
    ],
    "correctIndex": 2
  },
  {
    "id": 172,
    "question": "Delta Sharing across clouds and regions incurs additional costs for which scenario?",
    "options": [
      "Sharing within the same cloud and region",
      "Transferring across clouds and different regions",
      "Accessing via VPN within the same data center",
      "Internal analytics within a single cloud"
    ],
    "correctIndex": 1
  },
  {
    "id": 173,
    "question": "Which command enforces ingestion to fail when new columns appear?",
    "options": [
      "failOnNewColumns",
      "none",
      "rescue",
      "addNewColumns"
    ],
    "correctIndex": 0
  },
  {
    "id": 174,
    "question": "Which SQL snippet is a DDL operation to create a table?",
    "options": [
      "CREATE TABLE employees (id INT, name STRING);",
      "DROP TABLE employees;",
      "ALTER TABLE employees ADD COLUMN salary DECIMAL(10,2);",
      "INSERT INTO employees (id, name) VALUES (1 'Alice');"
    ],
    "correctIndex": 0
  },
  {
    "id": 175,
    "question": "Which Databricks notebook feature helps organize steps into a scheduled pipeline?",
    "options": [
      "Real-time streaming support",
      "Collaborative editing",
      "Task workflows and job scheduling",
      "Notebook version control"
    ],
    "correctIndex": 2
  },
  {
    "id": 176,
    "question": "Which tool should be used to package and deploy version-controlled workflows with external schedulers?",
    "options": [
      "Databricks Connect",
      "Databricks Asset Bundles",
      "Databricks Command Line Interface",
      "Databricks Software Development Kit"
    ],
    "correctIndex": 1
  },
  {
    "id": 177,
    "question": "Which Databricks Asset Bundle format is valid?",
    "options": [
      "resources: jobs: hello-job: name: hello-job tasks: - task_key: hello-task existing_cluster_id: 1234-567890-abcde123 notebook_task: notebook_path: ./hello.py",
      "{ \"resources\": { \"jobs\": { \"name\": \"hello-job\", \"tasks\": { \"task_key\": \"hello-task\", \"existing_cluster_id\": \"1234-567890-abcde123\", \"notebook_task\": { \"notebook_path\": \".hello.py\" } } } } }",
      "configuration = { \"resources\": { \"jobs\": { \"name\": \"hello-job\", \"tasks\": { \"task_key\": \"hello-task\", \"existing_cluster_id\": \"1234-567890-abcde123\", \"notebook_task\": { \"notebook_path\": \".hello.py\" } } } } }",
      "resources { jobs { name = \"hello-job\" tasks { task_key = \"hello-task\" existing_cluster_id = \"1234-567890-abcde123\" notebook_task { notebook_path = \".hello.py\" } } } }"
    ],
    "correctIndex": 0
  },
  {
    "id": 178,
    "question": "A streaming pipeline should drop invalid records without failing. Which DLT feature does this?",
    "options": [
      "Change Data Capture",
      "Error Handling",
      "Monitoring",
      "Expectations"
    ],
    "correctIndex": 3
  },
  {
    "id": 179,
    "question": "A pipeline is under-utilized and data is skewed. How should it be fixed?",
    "options": [
      "Coalesce the dataset",
      "Increase executors",
      "Repartition the dataset to spread data across nodes",
      "Increase executor memory"
    ],
    "correctIndex": 2
  },
  {
    "id": 180,
    "question": "Which technology provides ACID transactions and schema enforcement?",
    "options": [
      "Delta Lake",
      "Unity Catalog",
      "Cloud File Storage",
      "Data lake"
    ],
    "correctIndex": 0
  },
  {
    "id": 181,
    "question": "Which transformation is typical for the Bronze layer?",
    "options": [
      "Include columns like load date/time and process ID",
      "Business rules and transformations",
      "Perform extensive data cleansing",
      "Aggregate data from multiple sources"
    ],
    "correctIndex": 0
  },
  {
    "id": 182,
    "question": "Which feature enables querying external systems (MySQL, Redshift, BigQuery) without ingesting into Databricks?",
    "options": [
      "Delta Lake",
      "Lakehouse Federation",
      "MLflow",
      "Databricks Connect"
    ],
    "correctIndex": 1
  },
  {
    "id": 183,
    "question": "Which method securely shares Unity Catalog data with a non-Databricks partner?",
    "options": [
      "Delta Sharing with the open sharing protocol",
      "Export CSV and email",
      "Use a third-party API",
      "Databricks-to-Databricks Sharing"
    ],
    "correctIndex": 0
  },
  {
    "id": 184,
    "question": "Why is Delta Live Tables appropriate for pipelines needing quality checks and schema evolution?",
    "options": [
      "Automatic quality checks, schema evolution, and declarative pipelines",
      "Manual schema enforcement with high overhead",
      "No streaming support and complex maintenance",
      "Batch-only processing with high costs"
    ],
    "correctIndex": 0
  },
  {
    "id": 185,
    "question": "Why does mixing Python and SQL in the same cell fail in Databricks?",
    "options": [
      "Only Scala and SQL can interoperate in the same cell",
      "Multiple languages are allowed but only one per notebook",
      "Only one language per cell is supported",
      "Interoperability requires a special character"
    ],
    "correctIndex": 2
  },
  {
    "id": 186,
    "question": "Which compute option fits small ad-hoc Python scripts that run frequently and should shut down quickly?",
    "options": [
      "All-purpose Cluster",
      "Job Cluster",
      "Serverless Compute",
      "SQL Warehouse"
    ],
    "correctIndex": 2
  },
  {
    "id": 187,
    "question": "How does Databricks Connect enable local development with clusters?",
    "options": [
      "Requires a specific IDE",
      "Only works via Databricks web interface",
      "Runs Spark jobs locally without network",
      "Mimics Databricks runtime for local dev in preferred IDE"
    ],
    "correctIndex": 3
  },
  {
    "id": 188,
    "question": "Where should historical Kafka event data be stored in a medallion architecture to be cost mindful?",
    "options": [
      "Gold",
      "Silver",
      "Bronze",
      "Raw layer"
    ],
    "correctIndex": 2
  },
  {
    "id": 189,
    "question": "Which two items are characteristics of the Gold layer? (Choose two.)",
    "options": [
      "Historical lineage",
      "Raw data",
      "Normalised",
      "De-normalised",
      "Read-optimized"
    ],
    "multiSelect": true,
    "correctIndices": [
      3,
      4
    ]
  },
  {
    "id": 190,
    "question": "How can a data engineer confirm OPTIMIZE was executed on a Delta table?",
    "options": [
      "Check system.storage.predictive_optimization_operations_history",
      "SHOW TABLES EXTENDED to check partitions",
      "DESCRIBE DETAIL to see file size and number of files",
      "DESCRIBE HISTORY to see OPTIMIZE operations"
    ],
    "correctIndex": 3
  },
  {
    "id": 191,
    "question": "Which cluster is cheapest and efficient for a small 10GB workload with simple joins?",
    "options": [
      "Interactive cluster",
      "Job cluster with spot instances enabled",
      "Job cluster with spot instances disabled",
      "Job cluster with Photon enabled"
    ],
    "correctIndex": 1
  },
  {
    "id": 192,
    "question": "Which syntax loads JSON files as they arrive using Auto Loader?",
    "options": [
      "df = spark.read.json(\"input/path\")",
      "df = spark.readStream.format(\"cloud\").option(\"json\").load(\"/input/path\")",
      "df = spark.readStream.format(\"json\").load(\"input/path\")",
      "df = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"json\").load(\"/input/path\")"
    ],
    "correctIndex": 3
  },
  {
    "id": 193,
    "question": "Which layer pairing is appropriate in a medallion architecture?",
    "options": [
      "Silver layer - Raw deposit account data",
      "Bronze layer - Summary of cash deposit amount per country and city",
      "Silver layer - Cleansed master customer data",
      "Gold layer - Deduplicated money transfer transaction"
    ],
    "correctIndex": 2
  },
  {
    "id": 194,
    "question": "How does Unity Catalog lineage visualize relationships?",
    "options": [
      "Visualizes dependencies between Delta tables, notebooks, and jobs without dashboards or column-level tracing",
      "Only supports table-level relationships and excludes notebooks, jobs, and dashboards",
      "Tracks dependencies between tables and notebooks but excludes jobs and dashboards",
      "Visualizes dependencies between tables, notebooks, jobs, and dashboards with column-level tracking"
    ],
    "correctIndex": 1
  },
  {
    "id": 195,
    "question": "Which SQL Warehouse should be used for large numbers of queries quickly and cost-effectively in a custom network?",
    "options": [
      "Serverless compute for notebooks",
      "Pro SQL Warehouse",
      "Classic SQL Warehouse",
      "Serverless SQL Warehouse"
    ],
    "correctIndex": 1
  },
  {
    "id": 196,
    "question": "Which role allows granting/revoking privileges in a schema without read/write access?",
    "options": [
      "Table Owner",
      "Catalog Owner",
      "Schema Owner",
      "USE catalog/schema privilege"
    ],
    "correctIndex": 2
  },
  {
    "id": 197,
    "question": "Which tool should be used to debug a PySpark notebook and inspect variable values?",
    "options": [
      "Databricks CLI to analyze driver logs",
      "Python Notebook Interactive Debugger",
      "Ganglia UI",
      "Spark UI"
    ],
    "correctIndex": 1
  },
  {
    "id": 198,
    "question": "How should an external table be created to reference ADLS data without moving it?",
    "options": [
      "CREATE MANAGED TABLE with LOCATION",
      "CREATE UNMANAGED TABLE without LOCATION",
      "CREATE TABLE with LOCATION pointing to external data",
      "CREATE EXTERNAL TABLE without LOCATION"
    ],
    "correctIndex": 2
  },
  {
    "id": 199,
    "question": "Which cluster meets PoC needs with real-time results and reduced spikes?",
    "options": [
      "All-purpose cluster with autoscaling",
      "Job cluster with Photon and autoscaling",
      "Job cluster with autoscaling",
      "All-purpose cluster with large fixed memory"
    ],
    "correctIndex": 0
  },
  {
    "id": 200,
    "question": "Which compute option auto-scales for fluctuating SQL workloads and charges per use?",
    "options": [
      "Databricks SQL Analytics",
      "Databricks Runtime for ML",
      "Databricks Jobs",
      "Serverless SQL Warehouse"
    ],
    "correctIndex": 3
  },
  {
    "id": 201,
    "question": "How should a large Delta dataset be shared with a non-Databricks partner securely with read-only access?",
    "options": [
      "Export to CSV and transfer manually",
      "Grant workspace access with write permissions",
      "Share via Unity Catalog with full write access",
      "Share via Delta Sharing with a secure, read-only URL"
    ],
    "correctIndex": 3
  },
  {
    "id": 202,
    "question": "What happens when OPTIMIZE is run twice on the same Delta table with unchanged data?",
    "options": [
      "No effect because it is idempotent",
      "Significantly changes tuples per file",
      "Further reduces file sizes by re-clustering",
      "Triggers full liquid clustering"
    ],
    "correctIndex": 0
  },
  {
    "id": 203,
    "question": "What information is first required to set up Delta Sharing with a Unity Catalog partner?",
    "options": [
      "IP address of their workspace",
      "Name of their cluster",
      "Sharing identifier of their Unity Catalog metastore",
      "Databricks account password"
    ],
    "correctIndex": 2
  },
  {
    "id": 204,
    "question": "A costly workflow failed at the last stage. After fixing, what should be done to minimize cost and downtime?",
    "options": [
      "Re-run the entire workflow",
      "Repair run",
      "Restart the cluster",
      "Switch to another cluster"
    ],
    "correctIndex": 1
  },
  {
    "id": 205,
    "question": "Which approach ensures automatic scaling and minimal downtime with minimal manual cluster management?",
    "options": [
      "Job clusters with fixed configs and no auto-scaling",
      "Spot instances with potential interruptions",
      "Interactive clusters with manual scaling",
      "Serverless compute with automatic scaling"
    ],
    "correctIndex": 3
  },
  {
    "id": 206,
    "question": "A single-task workflow fails due to a notebook error and is fixed. What should be done to rerun?",
    "options": [
      "Repair the task",
      "Rerun the pipeline",
      "Restart the cluster",
      "Switch the cluster"
    ],
    "correctIndex": 0
  },
  {
    "id": 207,
    "question": "Which set of grants provides least privilege to create tables in manufacturing.quality?",
    "options": [
      "GRANT CREATE TABLE ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT USE SCHEMA ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT USE CATALOG ON CATALOG manufacturing TO manufacturing-team;",
      "GRANT USE TABLE ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT USE SCHEMA ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT USE CATALOG ON CATALOG manufacturing TO manufacturing-team;",
      "GRANT CREATE TABLE ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT CREATE SCHEMA ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT CREATE CATALOG ON CATALOG manufacturing TO manufacturing-team;",
      "GRANT CREATE TABLE ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT CREATE SCHEMA ON SCHEMA manufacturing.quality TO manufacturing-team; GRANT USE CATALOG ON CATALOG manufacturing TO manufacturing-team;"
    ],
    "correctIndex": 0
  },
  {
    "id": 208,
    "question": "High CPU time vs Task time indicates a CPU-bound job. What should be done?",
    "options": [
      "Repartition data because the cluster is under-utilized",
      "No change needed",
      "Tune executors/cores or resize the cluster",
      "Increase parallelism due to memory pressure"
    ],
    "correctIndex": 2
  },
  {
    "id": 209,
    "question": "Which Auto Loader code parses only .png files in a directory?",
    "options": [
      "df = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"binaryFile\").append(\"/*.png\")",
      "df = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"binaryFile\").option(\"pathGlobFilter\", \"*.png\").load()",
      "df = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"binaryFile\").option(\"pathGlobFilter\", \"*.png\").append()",
      "df = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"binaryFile\").load(\"/*.png\")"
    ],
    "correctIndex": 1
  },
  {
    "id": 210,
    "question": "Which languages are supported by Serverless compute clusters? (Choose two.)",
    "options": [
      "SQL",
      "Python",
      "R",
      "Scala",
      "Java"
    ],
    "multiSelect": true,
    "correctIndices": [
      0,
      1
    ]
  },
  {
    "id": 211,
    "question": "Spark SQL fails with OutOfMemoryError. Which two corrective actions should be taken? (Choose two.)",
    "options": [
      "Narrow filters to collect less data",
      "Upsize worker nodes and enable autoshuffle partitions",
      "Upsize driver node and deactivate autoshuffle partitions",
      "Cache the dataset",
      "Fix shuffle partitions to 50"
    ],
    "multiSelect": true,
    "correctIndices": [
      0,
      1
    ]
  },
  {
    "id": 212,
    "question": "One of the foundational technologies provided by the Databricks Lakehouse Platform is an open-source, file-based storage format that brings reliability to data lakes. Which of the following technologies is being described in the above statement?",
    "options": [
      "Delta Lives Tables (DLT)",
      "Delta Lake",
      "Apache Spark",
      "Unity Catalog",
      "Photon"
    ],
    "correctIndex": 1,
    "explanation": "Delta Lake is an open source technology that extends Parquet data files with a file-based transaction log for ACID transactions that brings reliability to data lakes.",
    "domain": "Databricks Lakehouse Platform"
  },
  {
    "id": 213,
    "question": "Which of the following commands can a data engineer use to purge stale data files of a Delta table?",
    "options": [
      "DELETE",
      "GARBAGE COLLECTION",
      "CLEAN",
      "VACUUM",
      "OPTIMIZE"
    ],
    "correctIndex": 3,
    "explanation": "The VACUUM command deletes the unused data files older than a specified data retention period.",
    "domain": "Databricks Lakehouse Platform"
  },
  {
    "id": 214,
    "question": "In Databricks Repos (Git folders), which of the following operations a data engineer can use to save local changes of a repo to its remote repository?",
    "options": [
      "Create Pull Request",
      "Commit & Pull",
      "Commit & Push",
      "Merge & Push",
      "Merge & Pull"
    ],
    "correctIndex": 2,
    "explanation": "Commit & Push is used to save the changes on a local repo, then uploads this local repo content to the remote repository.",
    "domain": "Databricks Lakehouse Platform"
  },
  {
    "id": 215,
    "question": "In Delta Lake tables, which of the following is the primary format for the transaction log files?",
    "options": [
      "Delta",
      "Parquet",
      "JSON",
      "Hive-specific format",
      "XML"
    ],
    "correctIndex": 2,
    "explanation": "Delta Lake builds upon standard data formats. Delta lake table gets stored on the storage in one or more data files in Parquet format, along with transaction logs in JSON format.",
    "domain": "Databricks Lakehouse Platform"
  },
  {
    "id": 216,
    "question": "Which of the following functionalities can be performed in Databricks Repos (Git folders)?",
    "options": [
      "Create pull requests",
      "Create new remote Git repositories",
      "Delete branches",
      "Create CI/CD pipelines",
      "Pull from a remote Git repository"
    ],
    "correctIndex": 4,
    "explanation": "Databricks Repos supports git Pull operation. It is used to fetch and download content from a remote repository and immediately update the local repo to match that content.",
    "domain": "Databricks Lakehouse Platform"
  },
  {
    "id": 217,
    "question": "Which of the following locations completely hosts the customer data?",
    "options": [
      "Customer's cloud account",
      "Control plane",
      "Databricks account",
      "Databricks-managed cluster",
      "Repos"
    ],
    "correctIndex": 0,
    "explanation": "According to the Databricks Lakehouse architecture, the storage account hosting the customer data is provisioned in the data plane in the Databricks customer's cloud account.",
    "domain": "Databricks Lakehouse Platform"
  },
  {
    "id": 218,
    "question": "If the default notebook language is Python, which of the following options a data engineer can use to run SQL commands in this Python Notebook?",
    "options": [
      "They need first to import the SQL library in a cell",
      "This is not possible! They need to change the default language of the notebook to SQL",
      "Databricks detects cells language automatically, so they can write SQL syntax in any cell",
      "They can add %language magic command at the start of a cell to force language detection.",
      "They can add %sql at the start of a cell."
    ],
    "correctIndex": 4,
    "explanation": "By default, cells use the default language of the notebook. You can override the default language in a cell by using the language magic command at the beginning of a cell. The supported magic commands are: %python, %sql, %scala, and %r.",
    "domain": "Databricks Lakehouse Platform"
  },
  {
    "id": 219,
    "question": "A junior data engineer uses the built-in Databricks Notebooks versioning for source control. A senior data engineer recommended using Databricks Repos (Git folders) instead. Which of the following could explain why Databricks Repos is recommended instead of Databricks Notebooks versioning?",
    "options": [
      "Databricks Repos supports creating and managing branches for development work.",
      "Databricks Repos automatically tracks the changes and keeps the history.",
      "Databricks Repos allows users to resolve merge conflicts",
      "Databricks Repos allows users to restore previous versions of a notebook",
      "All of these advantages explain why Databricks Repos is recommended instead of Notebooks versioning"
    ],
    "correctIndex": 0,
    "explanation": "One advantage of Databricks Repos over the built-in Databricks Notebooks versioning is that Databricks Repos supports creating and managing branches for development work.",
    "domain": "Databricks Lakehouse Platform"
  },
  {
    "id": 220,
    "question": "Which of the following services provides a data warehousing experience to its users?",
    "options": [
      "Databricks SQL",
      "Databricks Machine Learning",
      "Data Science and Engineering Workspace",
      "Unity Catalog",
      "Delta Lives Tables (DLT)"
    ],
    "correctIndex": 0,
    "explanation": "Databricks SQL (DB SQL) is a data warehouse on the Databricks Lakehouse Platform that lets you run all your SQL and BI applications at scale.",
    "domain": "Databricks Lakehouse Platform"
  },
  {
    "id": 221,
    "question": "A data engineer noticed that there are unused data files in the directory of a Delta table. They executed the VACUUM command on this table; however, only some of those unused data files have been deleted. Which of the following could explain why only some of the unused data files have been deleted after running the VACUUM command?",
    "options": [
      "The deleted data files were larger than the default size threshold. While the remaining files are smaller than the default size threshold and can not be deleted.",
      "The deleted data files were smaller than the default size threshold. While the remaining files are larger than the default size threshold and can not be deleted.",
      "The deleted data files were older than the default retention threshold. While the remaining files are newer than the default retention threshold and can not be deleted.",
      "The deleted data files were newer than the default retention threshold. While the remaining files are older than the default retention threshold and can not be deleted.",
      "More information is needed to determine the correct answer"
    ],
    "correctIndex": 2,
    "explanation": "Running the VACUUM command on a Delta table deletes the unused data files older than a specified data retention period. Unused files newer than the default retention threshold are kept untouched.",
    "domain": "Databricks Lakehouse Platform"
  },
  {
    "id": 222,
    "question": "The data engineering team has a Delta table called products that contains products' details including the net price. Which of the following code blocks will apply a 50% discount on all the products where the price is greater than 1000 and save the new price to the table?",
    "options": [
      "UPDATE products SET price = price * 0.5 WHERE price >= 1000;",
      "SELECT price * 0.5 AS new_price FROM products WHERE price > 1000;",
      "MERGE INTO products WHERE price < 1000 WHEN MATCHED UPDATE price = price * 0.5;",
      "UPDATE products SET price = price * 0.5 WHERE price > 1000;",
      "MERGE INTO products WHERE price > 1000 WHEN MATCHED UPDATE price = price * 0.5;"
    ],
    "correctIndex": 3,
    "explanation": "The UPDATE statement is used to modify the existing records in a table that match the WHERE condition. In this case, we are updating the products where the price is strictly greater than 1000.",
    "domain": "Databricks Lakehouse Platform"
  },
  {
    "id": 223,
    "question": "A data engineer wants to create a relational object by pulling data from two tables. The relational object will only be used in the current session. In order to save on storage costs, the data engineer wants to avoid copying and storing physical data. Which of the following relational objects should the data engineer create?",
    "options": [
      "External table",
      "Temporary view",
      "Managed table",
      "Global Temporary view",
      "View"
    ],
    "correctIndex": 1,
    "explanation": "In order to avoid copying and storing physical data, the data engineer must create a view object. A view in databricks is a virtual table that has no physical data. It's just a saved SQL query against actual tables. The view type should be Temporary view since it's tied to a Spark session and dropped when the session ends.",
    "domain": "ELT with Spark SQL and Python"
  },
  {
    "id": 224,
    "question": "A data engineer has a database named db_hr, and they want to know where this database was created in the underlying storage. Which of the following commands can the data engineer use to complete this task?",
    "options": [
      "DESCRIBE db_hr",
      "DESCRIBE EXTENDED db_hr",
      "DESCRIBE DATABASE db_hr",
      "SELECT location FROM db_hr.db",
      "There is no need for a command since all databases are created under the default hive metastore directory"
    ],
    "correctIndex": 2,
    "explanation": "The DESCRIBE DATABASE or DESCRIBE SCHEMA returns the metadata of an existing database (schema). The metadata information includes the database's name, comment, and location on the filesystem. If the optional EXTENDED option is specified, database properties are also returned.",
    "domain": "ELT with Spark SQL and Python"
  },
  {
    "id": 225,
    "question": "Which of the following commands a data engineer can use to register the table orders from an existing SQLite database?",
    "options": [
      "CREATE TABLE orders USING sqlite OPTIONS (url \"jdbc:sqlite:/bookstore.db\", dbtable \"orders\")",
      "CREATE TABLE orders USING org.apache.spark.sql.jdbc OPTIONS (url \"jdbc:sqlite:/bookstore.db\", dbtable \"orders\")",
      "CREATE TABLE orders USING cloudfiles OPTIONS (url \"jdbc:sqlite:/bookstore.db\", dbtable \"orders\")",
      "CREATE TABLE orders USING EXTERNAL OPTIONS (url \"jdbc:sqlite:/bookstore.db\", dbtable \"orders\")",
      "CREATE TABLE orders USING DATABASE OPTIONS (url \"jdbc:sqlite:/bookstore.db\", dbtable \"orders\")"
    ],
    "correctIndex": 1,
    "explanation": "Using the JDBC library, Spark SQL can extract data from any existing relational database that supports JDBC. Examples include mysql, postgres, SQLite, and more.",
    "domain": "ELT with Spark SQL and Python"
  },
  {
    "id": 226,
    "question": "When dropping a Delta table, which of the following explains why both the table's metadata and the data files will be deleted?",
    "options": [
      "The table is shallow cloned",
      "The table is external",
      "The user running the command has the necessary permissions to delete the data files",
      "The table is managed",
      "The data files are older than the default retention period"
    ],
    "correctIndex": 3,
    "explanation": "Managed tables are tables whose metadata and the data are managed by Databricks. When you run DROP TABLE on a managed table, both the metadata and the underlying data files are deleted.",
    "domain": "ELT with Spark SQL and Python"
  },
  {
    "id": 227,
    "question": "Given the following commands: CREATE DATABASE db_hr; USE db_hr; CREATE TABLE employees; In which of the following locations will the employees table be located?",
    "options": [
      "dbfs:/user/hive/warehouse",
      "dbfs:/user/hive/warehouse/db_hr.db",
      "dbfs:/user/hive/warehouse/db_hr",
      "dbfs:/user/hive/databases/db_hr.db",
      "More information is needed to determine the correct answer"
    ],
    "correctIndex": 1,
    "explanation": "Since we are creating the database here without specifying a LOCATION clause, the database will be created in the default warehouse directory under dbfs:/user/hive/warehouse. The database folder have the extension (.db) And since we are creating the table also without specifying a LOCATION clause, the table becomes a managed table created under the database directory (in db_hr.db folder)",
    "domain": "ELT with Spark SQL and Python"
  },
  {
    "id": 228,
    "question": "Which of the following code blocks can a data engineer use to create a Python function to multiply two integers and return the result?",
    "options": [
      "def multiply_numbers(num1, num2):\n    print(num1 * num2)",
      "def fun: multiply_numbers(num1, num2):\n    return num1 * num2",
      "def multiply_numbers(num1, num2):\n    return num1 * num2",
      "fun multiply_numbers(num1, num2):\n    return num1 * num2",
      "fun def multiply_numbers(num1, num2):\n    return num1 * num2"
    ],
    "correctIndex": 2,
    "explanation": "In Python, a function is defined using the def keyword. Here, we used the return keyword since the question clearly asks to return the result, and not printing the output.",
    "domain": "ELT with Spark SQL and Python"
  },
  {
    "id": 229,
    "question": "Given the following 2 tables, fill in the blank to make the following query return the result showing all students and their course IDs: SELECT students.name, students.age, enrollments.course_id FROM students _____________ enrollments ON students.student_id = enrollments.student_id",
    "options": [
      "RIGHT JOIN",
      "LEFT JOIN",
      "INNER JOIN",
      "ANTI JOIN",
      "CROSS JOIN"
    ],
    "correctIndex": 1,
    "explanation": "LEFT JOIN returns all values from the left table and the matched values from the right table, or appends NULL if there is no match. In the above example, we see NULL in the course_id of John (U0003) since he is not enrolled in any course.",
    "domain": "ELT with Spark SQL and Python"
  },
  {
    "id": 230,
    "question": "Which of the following SQL keywords can be used to rotate rows of a table by turning row values into multiple columns?",
    "options": [
      "ROTATE",
      "TRANSFORM",
      "PIVOT",
      "GROUP BY",
      "ZORDER BY"
    ],
    "correctIndex": 2,
    "explanation": "PIVOT transforms the rows of a table by rotating unique values of a specified column list into separate columns. In other words, It converts a table from a long format to a wide format.",
    "domain": "ELT with Spark SQL and Python"
  },
  {
    "id": 231,
    "question": "Fill in the below blank to get the number of courses incremented by 1 for each student in array column students: SELECT faculty_id, students, ___________ AS new_totals FROM faculties",
    "options": [
      "TRANSFORM (students, total_courses + 1)",
      "TRANSFORM (students, i -> i.total_courses + 1)",
      "FILTER (students, total_courses + 1)",
      "FILTER (students, i -> i.total_courses + 1)",
      "CASE WHEN students.total_courses IS NOT NULL THEN students.total_courses + 1 ELSE NULL END"
    ],
    "correctIndex": 1,
    "explanation": "transform(input_array, lambd_function) is a higher order function that returns an output array from an input array by transforming each element in the array using a given lambda function.",
    "domain": "ELT with Spark SQL and Python"
  },
  {
    "id": 232,
    "question": "Fill in the below blank to successfully create a table using data from CSV files located at /path/input: CREATE TABLE my_table (col1 STRING, col2 STRING) ____________ OPTIONS (header = \"true\", delimiter = \";\") LOCATION = \"/path/input\"",
    "options": [
      "FROM CSV",
      "USING CSV",
      "USING DELTA",
      "AS",
      "AS CSV"
    ],
    "correctIndex": 1,
    "explanation": "CREATE TABLE USING allows to specify an external data source type like CSV format, and with any additional options. This creates an external table pointing to files stored in an external location.",
    "domain": "ELT with Spark SQL and Python"
  },
  {
    "id": 233,
    "question": "Which of the following statements best describes the usage of CREATE SCHEMA command?",
    "options": [
      "It's used to create a table schema (columns names and datatype)",
      "It's used to create a Hive catalog",
      "It's used to infer and store schema in \"cloudFiles.schemaLocation\"",
      "It's used to create a database",
      "It's used to merge the schema when writing data into a target table"
    ],
    "correctIndex": 3,
    "explanation": "CREATE SCHEMA is an alias for CREATE DATABASE statement. While usage of SCHEMA and DATABASE is interchangeable, SCHEMA is preferred.",
    "domain": "ELT with Spark SQL and Python"
  },
  {
    "id": 234,
    "question": "Which of the following statements is Not true about CTAS statements?",
    "options": [
      "CTAS statements automatically infer schema information from query results",
      "CTAS statements support manual schema declaration",
      "CTAS statements stand for CREATE TABLE _ AS SELECT statement",
      "With CTAS statements, data will be inserted during the table creation",
      "All these statements are Not true about CTAS statements"
    ],
    "correctIndex": 1,
    "explanation": "CREATE TABLE AS SELECT statements, or CTAS statements create and populate Delta tables using the output of a SELECT query. CTAS statements automatically infer schema information from query results and do not support manual schema declaration.",
    "domain": "ELT with Spark SQL and Python"
  },
  {
    "id": 235,
    "question": "Which of the following SQL commands will append this new row to the existing Delta table users?",
    "options": [
      "APPEND INTO users VALUES (\"0015\", \"Adam\", 23)",
      "INSERT VALUES (\"0015\", \"Adam\", 23) INTO users",
      "APPEND VALUES (\"0015\", \"Adam\", 23) INTO users",
      "INSERT INTO users VALUES (\"0015\", \"Adam\", 23)",
      "UPDATE users VALUES (\"0015\", \"Adam\", 23)"
    ],
    "correctIndex": 3,
    "explanation": "INSERT INTO allows inserting new rows into a Delta table. You specify the inserted rows by value expressions or the result of a query.",
    "domain": "ELT with Spark SQL and Python"
  },
  {
    "id": 236,
    "question": "Given the following Structured Streaming query: (spark.table(\"orders\").withColumn(\"total_after_tax\", col(\"total\")+col(\"tax\")).writeStream.option(\"checkpointLocation\", checkpointPath).outputMode(\"append\").___________table(\"new_orders\")). Fill in the blank to make the query executes multiple micro-batches to process all available data, then stops the trigger.",
    "options": [
      "trigger(\"micro-batches\")",
      "trigger(once=True)",
      "trigger(processingTime=\"0 seconds\")",
      "trigger(micro-batches=True)",
      "trigger(availableNow=True)"
    ],
    "correctIndex": 4,
    "explanation": "In Spark Structured Streaming, we use trigger(availableNow=True) to run the stream in batch mode where it processes all available data in multiple micro-batches. The trigger will stop on its own once it finishes processing the available data.",
    "domain": "Incremental Data Processing"
  },
  {
    "id": 237,
    "question": "Which of the following techniques allows Auto Loader to track the ingestion progress and store metadata of the discovered files?",
    "options": [
      "mergeSchema",
      "COPY INTO",
      "Watermarking",
      "Checkpointing",
      "Z-Ordering"
    ],
    "correctIndex": 3,
    "explanation": "Auto Loader keeps track of discovered files using checkpointing in the checkpoint location. Checkpointing allows Auto loader to provide exactly-once ingestion guarantees.",
    "domain": "Incremental Data Processing"
  },
  {
    "id": 238,
    "question": "A data engineer has defined the following data quality constraint in a Delta Live Tables pipeline: CONSTRAINT valid_id EXPECT (id IS NOT NULL) _____________. Fill in the above blank so records violating this constraint cause the pipeline to fail.",
    "options": [
      "ON VIOLATION FAIL",
      "ON VIOLATION FAIL UPDATE",
      "ON VIOLATION DROP ROW",
      "ON VIOLATION FAIL PIPELINE",
      "There is no need to add ON VIOLATION clause. By default, records violating the constraint cause the pipeline to fail."
    ],
    "correctIndex": 1,
    "explanation": "With ON VIOLATION FAIL UPDATE, records that violate the expectation will cause the pipeline to fail. When a pipeline fails because of an expectation violation, you must fix the pipeline code to handle the invalid data correctly before re-running the pipeline.",
    "domain": "Incremental Data Processing"
  },
  {
    "id": 239,
    "question": "In multi-hop architecture, which of the following statements best describes the Silver layer tables?",
    "options": [
      "They maintain data that powers analytics, machine learning, and production applications",
      "They maintain raw data ingested from various sources",
      "The table structure in this layer resembles that of the source system table structure with any additional metadata columns like the load time, and input file name.",
      "They provide business-level aggregated version of data",
      "They provide a more refined view of raw data, where it's filtered, cleaned, and enriched."
    ],
    "correctIndex": 4,
    "explanation": "Silver tables provide a more refined view of the raw data. For example, data can be cleaned and filtered at this level. And we can also join fields from various bronze tables to enrich our silver records.",
    "domain": "Incremental Data Processing"
  },
  {
    "id": 240,
    "question": "The data engineer team has a DLT pipeline that updates all the tables at defined intervals until manually stopped. The compute resources of the pipeline continue running to allow for quick testing. Which of the following best describes the execution modes of this DLT pipeline?",
    "options": [
      "The DLT pipeline executes in Continuous Pipeline mode under Production mode.",
      "The DLT pipeline executes in Continuous Pipeline mode under Development mode.",
      "The DLT pipeline executes in Triggered Pipeline mode under Production mode.",
      "The DLT pipeline executes in Triggered Pipeline mode under Development mode.",
      "More information is needed to determine the correct response"
    ],
    "correctIndex": 1,
    "explanation": "Continuous pipelines update tables continuously as input data changes. Once an update is started, it continues to run until the pipeline is shut down. In Development mode, the Delta Live Tables system ease the development process by reusing a cluster to avoid the overhead of restarts. The cluster runs for two hours when development mode is enabled. Disabling pipeline retries so you can immediately detect and fix errors.",
    "domain": "Incremental Data Processing"
  },
  {
    "id": 241,
    "question": "Given the following Structured Streaming query: (spark.readStream.table(\"cleanedOrders\").groupBy(\"productCategory\").agg(sum(\"totalWithTax\")).writeStream.option(\"checkpointLocation\", checkpointPath).outputMode(\"complete\").table(\"aggregatedOrders\")). Which of the following best describe the purpose of this query in a multi-hop architecture?",
    "options": [
      "The query is performing raw data ingestion into a Bronze table",
      "The query is performing a hop from a Bronze table to a Silver table",
      "The query is performing a hop from Silver layer to a Gold table",
      "The query is performing data transfer from a Gold table into a production application",
      "This query is performing data quality controls prior to Silver layer"
    ],
    "correctIndex": 2,
    "explanation": "The above Structured Streaming query creates business-level aggregates from clean orders data in the silver table cleanedOrders, and loads them in the gold table aggregatedOrders.",
    "domain": "Incremental Data Processing"
  },
  {
    "id": 242,
    "question": "Given the following Structured Streaming query: (spark.readStream.table(\"orders\").writeStream.option(\"checkpointLocation\", checkpointPath).table(\"Output_Table\")). Which of the following is the trigger Interval for this query?",
    "options": [
      "Every half second",
      "Every half min",
      "Every half hour",
      "The query will run in batch mode to process all available data at once, then the trigger stops.",
      "More information is needed to determine the correct response"
    ],
    "correctIndex": 0,
    "explanation": "By default, if you don't provide any trigger interval, the data will be processed every half second. This is equivalent to trigger(processingTime=\"500ms\")",
    "domain": "Incremental Data Processing"
  },
  {
    "id": 243,
    "question": "A data engineer has the following query in a Delta Live Tables pipeline: CREATE STREAMING TABLE sales_silver AS SELECT store_id, total + tax AS total_after_tax FROM LIVE.sales_bronze. The pipeline is failing to start due to an error in this query. Which of the following changes should be made to this query to successfully start the DLT pipeline?",
    "options": [
      "CREATE LIVE TABLE sales_silver AS SELECT store_id, total + tax AS total_after_tax FROM STREAMING(LIVE.sales_bronze)",
      "CREATE STREAMING TABLE sales_silver AS SELECT store_id, total + tax AS total_after_tax FROM LIVE(STREAM.sales_bronze)",
      "CREATE STREAMING TABLE sales_silver AS SELECT store_id, total + tax AS total_after_tax FROM STREAM(sales_bronze)",
      "CREATE STREAMING TABLE sales_silver AS SELECT store_id, total + tax AS total_after_tax FROM STREAMING(LIVE.sales_bronze)",
      "CREATE STREAMING TABLE sales_silver AS SELECT store_id, total + tax AS total_after_tax FROM STREAM(LIVE.sales_bronze)"
    ],
    "correctIndex": 4,
    "explanation": "In DLT pipelines, You can stream data from other tables in the same pipeline by using the STREAM() function. In this case, you must define a streaming table using the CREATE STREAMING TABLE syntax. Remember, to query another DLT table, prepend always the LIVE. keyword to the table name.",
    "domain": "Incremental Data Processing"
  },
  {
    "id": 244,
    "question": "In multi-hop architecture, which of the following statements best describes the Gold layer tables?",
    "options": [
      "They provide a more refined view of the data",
      "They maintain raw data ingested from various sources",
      "The table structure in this layer resembles that of the source system table structure with any additional metadata columns like the load time, and input file name.",
      "They provide business-level aggregations that power analytics, machine learning, and production applications",
      "They represent a filtered, cleaned, and enriched version of data"
    ],
    "correctIndex": 3,
    "explanation": "Gold layer is the final layer in the multi-hop architecture, where tables provide business level aggregates often used for reporting and dashboarding, or even for Machine learning.",
    "domain": "Incremental Data Processing"
  },
  {
    "id": 245,
    "question": "The data engineer team has a DLT pipeline that updates all the tables once and then stops. The compute resources of the pipeline terminate when the pipeline is stopped. Which of the following best describes the execution modes of this DLT pipeline?",
    "options": [
      "The DLT pipeline executes in Continuous Pipeline mode under Production mode.",
      "The DLT pipeline executes in Continuous Pipeline mode under Development mode.",
      "The DLT pipeline executes in Triggered Pipeline mode under Production mode.",
      "The DLT pipeline executes in Triggered Pipeline mode under Development mode.",
      "More information is needed to determine the correct response"
    ],
    "correctIndex": 2,
    "explanation": "Triggered pipelines update each table with whatever data is currently available and then they shut down. In Production mode, the Delta Live Tables system terminates the cluster immediately when the pipeline is stopped, restarts the cluster for recoverable errors, and retries execution in case of specific errors.",
    "domain": "Incremental Data Processing"
  },
  {
    "id": 246,
    "question": "A data engineer needs to determine whether to use Auto Loader or COPY INTO command in order to load input data files incrementally. In which of the following scenarios should the data engineer use Auto Loader over COPY INTO command?",
    "options": [
      "If they are going to ingest files in the order of millions or more over time",
      "If they are going to ingest few number of files in the order of thousands",
      "If they are going to load a subset of re-uploaded files",
      "If the data schema is not going to evolve frequently",
      "There is no difference between using Auto Loader and Copy Into command"
    ],
    "correctIndex": 0,
    "explanation": "If you're going to ingest files in the order of thousands, you can use COPY INTO. If you are expecting files in the order of millions or more over time, use Auto Loader. If your data schema is going to evolve frequently, Auto Loader provides better primitives around schema inference and evolution.",
    "domain": "Incremental Data Processing"
  },
  {
    "id": 247,
    "question": "From which of the following locations can a data engineer set a schedule to automatically refresh a Databricks SQL query?",
    "options": [
      "From the jobs UI",
      "From the SQL warehouses page in Databricks SQL",
      "From the Alerts page in Databricks SQL",
      "From the query's page in Databricks SQL",
      "There is no way to automatically refresh a query in Databricks SQL. Schedules can be set only for dashboards to refresh their underlying queries."
    ],
    "correctIndex": 3,
    "explanation": "In Databricks SQL, you can set a schedule to automatically refresh a query from the query's page.",
    "domain": "Production Pipelines"
  },
  {
    "id": 248,
    "question": "Databricks provides a declarative ETL framework for building reliable and maintainable data processing pipelines, while maintaining table dependencies and data quality. Which of the following technologies is being described above?",
    "options": [
      "Delta Live Tables",
      "Delta Lake",
      "Databricks Jobs",
      "Unity Catalog Lineage",
      "Databricks SQL"
    ],
    "correctIndex": 0,
    "explanation": "Delta Live Tables is a framework for building reliable, maintainable, and testable data processing pipelines. You define the transformations to perform on your data, and Delta Live Tables manages task orchestration, cluster management, monitoring, data quality, and error handling.",
    "domain": "Production Pipelines"
  },
  {
    "id": 249,
    "question": "Which of the following services can a data engineer use for orchestration purposes in Databricks platform?",
    "options": [
      "Delta Live Tables",
      "Cluster Pools",
      "Databricks Jobs",
      "Data Explorer",
      "Unity Catalog Lineage"
    ],
    "correctIndex": 2,
    "explanation": "Databricks Jobs allow to orchestrate data processing tasks. This means the ability to run and manage multiple tasks as a directed acyclic graph (DAG) in a job.",
    "domain": "Production Pipelines"
  },
  {
    "id": 250,
    "question": "A data engineer has a Job with multiple tasks that takes more than 2 hours to complete. In the last run, the final task unexpectedly failed. Which of the following actions can the data engineer perform to complete this Job Run while minimizing the execution time?",
    "options": [
      "They can rerun this Job Run to execute all the tasks",
      "They can repair this Job Run so only the failed tasks will be re-executed",
      "They need to delete the failed Run, and start a new Run for the Job",
      "They can keep the failed Run, and simply start a new Run for the Job",
      "They can run the Job in Production mode which automatically retries execution in case of errors"
    ],
    "correctIndex": 1,
    "explanation": "You can repair failed multi-task jobs by running only the subset of unsuccessful tasks and any dependent tasks. Because successful tasks are not re-run, this feature reduces the time and resources required to recover from unsuccessful job runs.",
    "domain": "Production Pipelines"
  },
  {
    "id": 251,
    "question": "A data engineering team has a multi-tasks Job in production. The team members need to be notified in the case of job failure. Which of the following approaches can be used to send emails to the team members in the case of job failure?",
    "options": [
      "They can use Job API to programmatically send emails according to each task status",
      "They can configure email notifications settings in the job page",
      "There is no way to notify users in the case of job failure",
      "Only Job owner can be configured to be notified in the case of job failure",
      "They can configure email notifications settings per notebook in the task page"
    ],
    "correctIndex": 1,
    "explanation": "Databricks Jobs support email notifications to be notified in the case of job start, success, or failure. Simply, click Edit email notifications from the details panel in the Job page. From there, you can add one or more email addresses.",
    "domain": "Production Pipelines"
  },
  {
    "id": 252,
    "question": "For production jobs, which of the following cluster types is recommended to use?",
    "options": [
      "All-purpose clusters",
      "Production clusters",
      "Job clusters",
      "On-premises clusters",
      "Serverless clusters"
    ],
    "correctIndex": 2,
    "explanation": "Job Clusters are dedicated clusters for a job or task run. A job cluster auto terminates once the job is completed, which saves cost compared to all-purpose clusters. In addition, Databricks recommends using job clusters in production so that each job runs in a fully isolated environment.",
    "domain": "Production Pipelines"
  },
  {
    "id": 253,
    "question": "In Databricks Jobs, which of the following approaches can a data engineer use to configure a linear dependency between Task A and Task B?",
    "options": [
      "They can select the Task A in the Depends On field of the Task B configuration",
      "They can assign Task A an Order number of 1, and assign Task B an Order number of 2",
      "They can visually drag and drop an arrow from Task A to Task B in the Job canvas",
      "They can configure the dependency at the notebook level using the dbutils.jobs utility",
      "Databricks Jobs do not support linear dependency between tasks. This can only be achieved in Delta Live Tables pipelines"
    ],
    "correctIndex": 0,
    "explanation": "You can define the order of execution of tasks in a job using the Depends on dropdown menu. You can set this field to one or more tasks in the job.",
    "domain": "Production Pipelines"
  },
  {
    "id": 254,
    "question": "Which part of the Databricks Platform can a data engineer use to revoke permissions from users on tables?",
    "options": [
      "Data Explorer",
      "Cluster event log",
      "Workspace Admin Console",
      "DBFS",
      "There is no way to revoke permissions in Databricks platform. The data engineer needs to clone the table with the updated permissions"
    ],
    "correctIndex": 0,
    "explanation": "Data Explorer in Databricks SQL allows you to manage data object permissions. This includes revoking privileges on tables and databases from users or groups of users.",
    "domain": "Data Governance"
  },
  {
    "id": 255,
    "question": "A data engineer uses the following SQL query: GRANT USAGE ON DATABASE sales_db TO finance_team. Which of the following is the benefit of the USAGE privilege?",
    "options": [
      "Gives read access on the database",
      "Gives full permissions on the entire database",
      "Gives the ability to view database objects and their metadata",
      "No effect! but it's required to perform any action on the database",
      "USAGE privilege is not part of the Databricks governance model"
    ],
    "correctIndex": 3,
    "explanation": "The USAGE does not give any abilities, but it's an additional requirement to perform any action on a schema (database) object.",
    "domain": "Data Governance"
  },
  {
    "id": 256,
    "question": "In which of the following locations can a data engineer change the owner of a table?",
    "options": [
      "In DBFS, from the properties tab of the table's data files",
      "In Data Explorer, under the Permissions tab of the table's page",
      "In Data Explorer, from the Owner field in the table's page",
      "In Data Explorer, under the Permissions tab of the database's page, since owners are set at database-level",
      "In Data Explorer, from the Owner field in the database's page, since owners are set at database-level"
    ],
    "correctIndex": 2,
    "explanation": "In Data Explorer, you can change the owner of a table from the Owner field in the table's page.",
    "domain": "Data Governance"
  }
]